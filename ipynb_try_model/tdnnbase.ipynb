{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T04:32:46.333788Z",
     "iopub.status.busy": "2022-06-21T04:32:46.333430Z",
     "iopub.status.idle": "2022-06-21T04:32:48.350952Z",
     "shell.execute_reply": "2022-06-21T04:32:48.349923Z",
     "shell.execute_reply.started": "2022-06-21T04:32:46.333741Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import librosa.display\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import argparse\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T04:32:48.353220Z",
     "iopub.status.busy": "2022-06-21T04:32:48.352956Z",
     "iopub.status.idle": "2022-06-21T04:33:06.961639Z",
     "shell.execute_reply": "2022-06-21T04:33:06.960750Z",
     "shell.execute_reply.started": "2022-06-21T04:32:48.353181Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q wavencoder\n",
    "import wavencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T04:33:06.963985Z",
     "iopub.status.busy": "2022-06-21T04:33:06.963777Z",
     "iopub.status.idle": "2022-06-21T04:33:06.968345Z",
     "shell.execute_reply": "2022-06-21T04:33:06.967555Z",
     "shell.execute_reply.started": "2022-06-21T04:33:06.963956Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "RATE = '8000'\n",
    "\n",
    "CATES = ['female', 'male']\n",
    "\n",
    "BASE_ORIGINAL_TRAIN = '../input/gendersclassification-vdt-2022/dataset_v2/train'\n",
    "NUM_WORKERS = 2\n",
    "BASE_TRAIN = BASE_ORIGINAL_TRAIN\n",
    "INFER_ONLY = True # change this to False to train the model again\n",
    "BASE_PUBLIC_TEST = '../input/gendersclassification-vdt-2022/public-test/public-test/wav'\n",
    "BASE_PRIVATE_TEST = '../input/gendersclassification-vdt-2022/private-test/private-test/wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T04:33:06.969784Z",
     "iopub.status.busy": "2022-06-21T04:33:06.969556Z",
     "iopub.status.idle": "2022-06-21T04:33:07.009169Z",
     "shell.execute_reply": "2022-06-21T04:33:07.008443Z",
     "shell.execute_reply.started": "2022-06-21T04:33:06.969751Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyDatasetSTFT(Dataset):\n",
    "    def __init__(self, filenames, labels, transform=None, duration=2, data_type='spectral'):\n",
    "        assert len(filenames) == len(labels), \"Number of files != number of labels\"\n",
    "        self.fns = filenames\n",
    "        self.lbs = labels\n",
    "        self.transform = transform\n",
    "        self.duration = duration\n",
    "        self.noise_dataset_path = 'noise_dataset'\n",
    "        self.get_transforms = wavencoder.transforms.Compose([\n",
    "            wavencoder.transforms.PadCrop(pad_crop_length=duration * 8000, pad_position='center',\n",
    "                                          crop_position='random'),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fns[idx]\n",
    "\n",
    "        wav, samplerate = torchaudio.load(fname)\n",
    "        transformed_audio = self.get_transforms(wav)\n",
    "\n",
    "        feats = torchaudio.transforms.MFCC(n_mfcc=40, sample_rate=8000, log_mels=True)(transformed_audio)\n",
    "\n",
    "        return torch.squeeze(feats.T), self.lbs[idx], self.fns[idx]\n",
    "\n",
    "\n",
    "def get_fn_lbs():\n",
    "    lbs = []\n",
    "    fns = []\n",
    "    count = 0\n",
    "    for name_dir in os.listdir(BASE_TRAIN):\n",
    "        for i in os.listdir(os.path.join(BASE_TRAIN, name_dir)):\n",
    "            for j in os.listdir(os.path.join(BASE_TRAIN, name_dir, i)):\n",
    "                dur = librosa.get_duration(filename=os.path.join(BASE_TRAIN, name_dir, i, j));\n",
    "                if dur > 1:\n",
    "                    lbs.append(count)\n",
    "                    fns.append(os.path.join(BASE_TRAIN, name_dir, i, j))\n",
    "            print(name_dir, i, end='')\n",
    "            print(count)\n",
    "            count += 1\n",
    "\n",
    "    return lbs, fns\n",
    "\n",
    "\n",
    "# def get_fn_lbs():\n",
    "#     lbs = []\n",
    "#     fns = []\n",
    "#     sum_file = 0\n",
    "#     for name_dir in os.listdir(BASE_TRAIN):\n",
    "#         for i in os.listdir(os.path.join(BASE_TRAIN,name_dir)):\n",
    "#             if name_dir == 'male':\n",
    "#                 for j in os.listdir(os.path.join(BASE_TRAIN,name_dir,i)):\n",
    "#                     lbs.append(1)\n",
    "#                     fns.append(os.path.join(BASE_TRAIN,name_dir,i,j))\n",
    "#                     sum_file +=1\n",
    "#             if name_dir == 'female':\n",
    "#                 for j in os.listdir(os.path.join(BASE_TRAIN,name_dir,i)):\n",
    "#                     lbs.append(0)\n",
    "#                     fns.append(os.path.join(BASE_TRAIN,name_dir,i,j))\n",
    "#                     sum_file +=1\n",
    "\n",
    "#     return lbs,fns\n",
    "\n",
    "\n",
    "def get_fn_test():\n",
    "    lbs = []\n",
    "    fns = []\n",
    "    df = pd.read_csv(\"../input/gendersclassification-vdt-2022/public-test/public-test/test_files.txt\", header=None,\n",
    "                     names=[\"gender\", \"filename\"])\n",
    "    for index, row in df.iterrows():\n",
    "        if str(row['gender']) == 'M':\n",
    "            du = librosa.get_duration(filename=os.path.join(BASE_PUBLIC_TEST, str(row['filename'])))\n",
    "            if du > 1:\n",
    "                lbs.append(1)\n",
    "                fns.append(os.path.join(BASE_PUBLIC_TEST, str(row['filename'])))\n",
    "        elif str(row['gender']) == 'F':\n",
    "            du = librosa.get_duration(filename=os.path.join(BASE_PUBLIC_TEST, str(row['filename'])))\n",
    "            if du > 1:\n",
    "                lbs.append(0)\n",
    "                fns.append(os.path.join(BASE_PUBLIC_TEST, str(row['filename'])))\n",
    "    return lbs, fns\n",
    "\n",
    "\n",
    "def get_fn_submit():\n",
    "    fns = []\n",
    "    lbs = []\n",
    "    df = pd.read_csv(\"../input/gendersclassification-vdt-2022/private-test/private-test/test_files.txt\", header=None,\n",
    "                     names=[\"gender\", \"filename\"])\n",
    "    for index, row in df.iterrows():\n",
    "        if str(row['gender']) == 'M':\n",
    "            du = librosa.get_duration(filename=os.path.join(BASE_PRIVATE_TEST, str(row['filename'])))\n",
    "            if du > 1:\n",
    "                lbs.append(1)\n",
    "                fns.append(os.path.join(BASE_PRIVATE_TEST, str(row['filename'])))\n",
    "        elif str(row['gender']) == 'F':\n",
    "            du = librosa.get_duration(filename=os.path.join(BASE_PRIVATE_TEST, str(row['filename'])))\n",
    "            if du > 1:\n",
    "                lbs.append(0)\n",
    "                fns.append(os.path.join(BASE_PRIVATE_TEST, str(row['filename'])))\n",
    "    return lbs, fns\n",
    "\n",
    "\n",
    "def build_dataloaders(args):\n",
    "\n",
    "    # train\n",
    "    submit_lbs, submit_fns = get_fn_submit()\n",
    "\n",
    "    lbs, fns = get_fn_lbs()\n",
    "\n",
    "    train_fns, val_fns, train_lbs, val_lbs = train_test_split(fns, lbs, test_size=0.1, random_state=42, shuffle=True)\n",
    "    #     test_lbs, test_fns = get_fn_test()\n",
    "    train_fns, test_fns, train_lbs, test_lbs = train_test_split(train_fns, train_lbs, test_size=0.1, random_state=42,\n",
    "                                                                shuffle=True)\n",
    "    print('First val fn: {}'.format(val_fns[0]))\n",
    "    print(Counter(train_lbs))\n",
    "    print(Counter(val_lbs))\n",
    "\n",
    "    num_classes = len(set(train_lbs))\n",
    "    print('Total training files: {}'.format(len(train_fns)))\n",
    "    print('Total validation files: {}'.format(len(val_fns)))\n",
    "    print('Total classes: {}'.format(num_classes))\n",
    "\n",
    "    dsets = dict()\n",
    "    dsets['train'] = MyDatasetSTFT(train_fns, train_lbs, duration=args.duration)\n",
    "    dsets['val'] = MyDatasetSTFT(val_fns, val_lbs, duration=args.duration)\n",
    "    dsets['test'] = MyDatasetSTFT(test_fns, test_lbs, duration=args.duration)\n",
    "    dsets['submit'] = MyDatasetSTFT(submit_fns, submit_lbs, duration=args.duration)\n",
    "\n",
    "    dset_loaders = dict()\n",
    "    dset_loaders['train'] = DataLoader(dsets['train'],\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       # sampler = WeightedRandomSampler(durations, args.batch_size, replacement = False),\n",
    "                                       # sampler = StratifiedSampler_weighted_duration(train_fns, gamma = args.gamma),\n",
    "                                       # sampler = StratifiedSampler_weighted(train_lbs, batch_size = args.batch_size, gamma = args.gamma),\n",
    "                                       num_workers=NUM_WORKERS)\n",
    "\n",
    "    dset_loaders['val'] = DataLoader(dsets['val'],\n",
    "                                     batch_size=args.batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=NUM_WORKERS)\n",
    "\n",
    "    dset_loaders['test'] = DataLoader(dsets['test'],\n",
    "                                      batch_size=args.batch_size,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=NUM_WORKERS)\n",
    "\n",
    "    dset_loaders['submit'] = DataLoader(dsets['submit'],\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=NUM_WORKERS)\n",
    "\n",
    "    return dset_loaders, (train_fns, test_fns, val_fns, train_lbs, test_lbs, val_lbs, submit_lbs, submit_fns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T04:33:07.013559Z",
     "iopub.status.busy": "2022-06-21T04:33:07.013235Z",
     "iopub.status.idle": "2022-06-21T04:33:07.184949Z",
     "shell.execute_reply": "2022-06-21T04:33:07.183936Z",
     "shell.execute_reply.started": "2022-06-21T04:33:07.013518Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def second2str(second):\n",
    "    h = int(second/3600.)\n",
    "    second -= h*3600.\n",
    "    m = int(second/60.)\n",
    "    s = int(second - m*60)\n",
    "    return \"{:d}:{:02d}:{:02d} (s)\".format(h, m, s)\n",
    "\n",
    "def print_eta(t0, cur_iter, total_iter):\n",
    "    \"\"\"\n",
    "    print estimated remaining time\n",
    "    t0: beginning time\n",
    "    cur_iter: current iteration\n",
    "    total_iter: total iterations\n",
    "    \"\"\"\n",
    "    time_so_far = time() - t0\n",
    "    iter_done = cur_iter + 1\n",
    "    iter_left = total_iter - cur_iter - 1\n",
    "    second_left = time_so_far/float(iter_done) * iter_left\n",
    "    s0 = 'Epoch: '+ str(cur_iter + 1) + '/' + str(total_iter) + ', time so far: ' \\\n",
    "        + second2str(time_so_far) + ', estimated time left: ' + second2str(second_left)\n",
    "    print(s0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in Z.\n",
    "    each row of Z is a set of score.\n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "def loader_len(dset):\n",
    "    \"\"\"\n",
    "    return len of a DataLoader\n",
    "\n",
    "    dset: a DataLoader that return (data, lbs, fns) at a batch\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    for _, _, fns in dset:\n",
    "        res += len(fns)\n",
    "    return res\n",
    "\n",
    "\n",
    "def singlemodel_score(model, dset_loader, num_tests = 1):\n",
    "    \"\"\"\n",
    "    Use ONE pretrained model to predict score and probs each input in dset\n",
    "    Make multiples predictions and accumulate results\n",
    "\n",
    "    ----\n",
    "    INPUT:\n",
    "        model: a pretrained model\n",
    "        dset: a MyDatasetSTFT variable\n",
    "    OUTPUT:\n",
    "        pred_outputs: np array -- prediction results, sum of all ouput before softmax\n",
    "        pred_probs: np array -- prediction results, sum of all probability\n",
    "        fns: list of filenames in loader order\n",
    "    \"\"\"\n",
    "    num_files = loader_len(dset_loader)\n",
    "#     num_classes = get_num_classes(model)\n",
    "    num_classes = 6\n",
    "\n",
    "    # preds = np.zeros((num_files, n_tests))\n",
    "    total_scores = np.zeros((num_files, num_classes))\n",
    "    total_probs = np.zeros((num_files, num_classes))\n",
    "    fns = []\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "\n",
    "    for test in range(num_tests):\n",
    "        tot = 0\n",
    "        print('test {}/{}'.format(test + 1, num_tests))\n",
    "        start = 0\n",
    "        for batch_idx, (inputs, labels, fns0) in enumerate(dset_loader):\n",
    "            n = len(fns0)\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            output = output.view((-1, num_classes))\n",
    "            _, pred  = torch.max(output.data, 1)\n",
    "            # preds[start:start + n, test] = pred.data.cpu().numpy()\n",
    "            # pdb.set_trace()\n",
    "            total_scores[start:start + n, :] += output.data.cpu().numpy()\n",
    "            start += n\n",
    "            tot += len(fns0)\n",
    "            if test == 0: fns.extend(fns0)\n",
    "\n",
    "        total_probs += softmax_stable(total_scores)\n",
    "    return total_scores, total_probs, fns\n",
    "\n",
    "\n",
    "def singlemodel_class(model, dset_loader, num_tests = 1):\n",
    "    \"\"\"\n",
    "    predict label for dset_loader using one model\n",
    "    This one is done after calling sm_score(model, dset_loader, num_tests)\n",
    "    and get the torch.max(out.data, 1)\n",
    "    \"\"\"\n",
    "    total_scores, total_probs, fns = singlemodel_score(model, dset_loader, num_tests)\n",
    "    score_class = np.argmax(total_scores, axis = 1)\n",
    "    prob_class = np.argmax(total_probs, axis = 1)\n",
    "    return score_class, prob_class, fns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:05:39.551831Z",
     "iopub.status.busy": "2022-06-21T05:05:39.551551Z",
     "iopub.status.idle": "2022-06-21T05:05:39.570947Z",
     "shell.execute_reply": "2022-06-21T05:05:39.570044Z",
     "shell.execute_reply.started": "2022-06-21T05:05:39.551799Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Digital Mammography Training')\n",
    "\n",
    "parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('--net_type', default='resnet', type=str, help='model')\n",
    "parser.add_argument('--depth', default=50, choices = [11, 16, 19, 18, 34, 50, 152, 161, 169, 121, 201], type=int, help='depth of model')\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float, help='weight decay')\n",
    "parser.add_argument('--finetune', '-f', action='store_true', help='Fine tune pretrained model')\n",
    "parser.add_argument('--trainer', default='adam', type = str, help = 'optimizer')\n",
    "parser.add_argument('--duration', default= 2, type = float, help='time duration for each file in second')\n",
    "parser.add_argument('--n_tests', default=3, type = int, help='number of tests in valid set')\n",
    "parser.add_argument('--gender', '-g', action='store_true', help='classify gender')\n",
    "parser.add_argument('--accent', '-a', action='store_true', help='accent classifier')\n",
    "parser.add_argument('--random_state', '-r', default = 2, type = int, help='random state in train_test_split')\n",
    "\n",
    "parser.add_argument('--model_path', type=str, default = ' ')\n",
    "parser.add_argument('--gamma', default = 0.5, type = float)\n",
    "parser.add_argument('--batch_size', default=2048, type=int)\n",
    "parser.add_argument('--num_epochs', default=100, type=int,\n",
    "                    help='Number of epochs in training')\n",
    "parser.add_argument('--dropout_keep_prob', default=0.5, type=float)\n",
    "parser.add_argument('--check_after', default=5,\n",
    "                    type=int, help='check the network after check_after epoch')\n",
    "parser.add_argument('--train_from', default=1,\n",
    "                    choices=[0, 1],  # 0: from scratch, 1: from pretrained 1 (need model_path)\n",
    "                    type=int,\n",
    "                    help=\"training from beginning (1) or from the most recent ckpt (0)\")\n",
    "parser.add_argument('--learning_rate', default=1e-3,   type=float,  )\n",
    "parser.add_argument('--frozen_until', '-fu', type=int, default = -1,\n",
    "                    help=\"freeze until --frozen_util block\")\n",
    "parser.add_argument('--val_ratio', default=0.1, type=float,\n",
    "        help = \"number of training samples per class\")\n",
    "\n",
    "########################################################################################33\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:05:40.527788Z",
     "iopub.status.busy": "2022-06-21T05:05:40.527528Z",
     "iopub.status.idle": "2022-06-21T05:05:58.859165Z",
     "shell.execute_reply": "2022-06-21T05:05:58.857724Z",
     "shell.execute_reply.started": "2022-06-21T05:05:40.527759Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Data preparation\n",
      "female southside0\n",
      "female midside1\n",
      "female northside2\n",
      "male southside3\n",
      "male midside4\n",
      "male northside5\n",
      "First val fn: ../input/gendersclassification-vdt-2022/dataset_v2/train/male/southside/os4a8CZPgCA_181_8k.wav\n",
      "Counter({4: 2286, 1: 2141, 3: 2057, 2: 1953, 5: 1856, 0: 1471})\n",
      "Counter({3: 302, 1: 275, 4: 261, 2: 223, 5: 206, 0: 186})\n",
      "Total training files: 11764\n",
      "Total validation files: 1453\n",
      "Total classes: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "print('======================================================')\n",
    "print('Data preparation')\n",
    "dset_loaders, train_info = build_dataloaders(args)\n",
    "train_fns, semi_fns, val_fns, train_lbs, semi_lbs, val_lbs,submit_lbs,submit_fns = train_info\n",
    "num_classes = len(set(train_lbs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:06:02.141376Z",
     "iopub.status.busy": "2022-06-21T05:06:02.141087Z",
     "iopub.status.idle": "2022-06-21T05:06:02.167116Z",
     "shell.execute_reply": "2022-06-21T05:06:02.166337Z",
     "shell.execute_reply.started": "2022-06-21T05:06:02.141343Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TDNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=40,\n",
    "            output_dim=512,\n",
    "            context_size=5,\n",
    "            stride=1,\n",
    "            dilation=1,\n",
    "            batch_norm=True,\n",
    "            dropout_p=0.0,\n",
    "            padding=0\n",
    "    ):\n",
    "        super(TDNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dilation = dilation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.padding = padding\n",
    "\n",
    "        self.kernel = nn.Conv1d(self.input_dim,\n",
    "                                self.output_dim,\n",
    "                                self.context_size,\n",
    "                                stride=self.stride,\n",
    "                                padding=self.padding,\n",
    "                                dilation=self.dilation)\n",
    "\n",
    "        self.nonlinearity = nn.LeakyReLU()\n",
    "        self.batch_norm = batch_norm\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(output_dim)\n",
    "        self.drop = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input: size (batch, seq_len, input_features)\n",
    "        outpu: size (batch, new_seq_len, output_features)\n",
    "        '''\n",
    "        _, _, d = x.shape\n",
    "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
    "\n",
    "        x = self.kernel(x.transpose(1, 2))\n",
    "        x = self.nonlinearity(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class StatsPool(nn.Module):\n",
    "\n",
    "    def __init__(self, floor=1e-10, bessel=False):\n",
    "        super(StatsPool, self).__init__()\n",
    "        self.floor = floor\n",
    "        self.bessel = bessel\n",
    "\n",
    "    def forward(self, x):\n",
    "        means = torch.mean(x, dim=1)\n",
    "        _, t, _ = x.shape\n",
    "        if self.bessel:\n",
    "            t = t - 1\n",
    "        residuals = x - means.unsqueeze(1)\n",
    "        numerator = torch.sum(residuals ** 2, dim=1)\n",
    "        stds = torch.sqrt(torch.clamp(numerator, min=self.floor) / t)\n",
    "        x = torch.cat([means, stds], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SoftMax(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.W = nn.Parameter(torch.FloatTensor(num_classes, num_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "    def forward(self, input, label=None):\n",
    "        x = input\n",
    "        W = self.W\n",
    "        logits = F.linear(x, W)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class XTDNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            features_per_frame=40,\n",
    "            final_features=1500,\n",
    "            embed_features=512,\n",
    "            dropout_p=0.0,\n",
    "            batch_norm=True\n",
    "    ):\n",
    "        super(XTDNN, self).__init__()\n",
    "        self.features_per_frame = features_per_frame\n",
    "        self.final_features = final_features\n",
    "        self.embed_features = embed_features\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_norm = batch_norm\n",
    "        tdnn_kwargs = {'dropout_p': dropout_p, 'batch_norm': self.batch_norm}\n",
    "\n",
    "        self.frame1 = TDNN(input_dim=self.features_per_frame, output_dim=512, context_size=5, dilation=1, **tdnn_kwargs)\n",
    "        self.frame2 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=2, **tdnn_kwargs)\n",
    "        self.frame3 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=3, **tdnn_kwargs)\n",
    "        self.frame4 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1, **tdnn_kwargs)\n",
    "        self.frame5 = TDNN(input_dim=512, output_dim=self.final_features, context_size=1, dilation=1, **tdnn_kwargs)\n",
    "\n",
    "        self.tdnn_list = nn.Sequential(self.frame1, self.frame2, self.frame3, self.frame4, self.frame5)\n",
    "        self.statspool = StatsPool()\n",
    "\n",
    "        self.fc_embed = nn.Linear(self.final_features * 2, self.embed_features)\n",
    "        self.nl_embed = nn.LeakyReLU()\n",
    "        self.bn_embed = nn.BatchNorm1d(self.embed_features)\n",
    "        self.drop_embed = nn.Dropout(p=self.dropout_p)\n",
    "        self.soft_max = SoftMax(num_features=embed_features, num_classes=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tdnn_list(x)\n",
    "        x = self.statspool(x)\n",
    "        x = self.fc_embed(x)\n",
    "        x = self.nl_embed(x)\n",
    "        x = self.bn_embed(x)\n",
    "        x = self.drop_embed(x)\n",
    "        x = self.soft_max(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:06:03.053834Z",
     "iopub.status.busy": "2022-06-21T05:06:03.053564Z",
     "iopub.status.idle": "2022-06-21T05:06:03.102167Z",
     "shell.execute_reply": "2022-06-21T05:06:03.101445Z",
     "shell.execute_reply.started": "2022-06-21T05:06:03.053802Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = XTDNN()\n",
    "model = model.to(device)\n",
    "lr = args.learning_rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "################################\n",
    "N_train = len(train_lbs)\n",
    "N_valid = len(val_lbs)\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:06:04.033236Z",
     "iopub.status.busy": "2022-06-21T05:06:04.032986Z",
     "iopub.status.idle": "2022-06-21T05:06:04.086216Z",
     "shell.execute_reply": "2022-06-21T05:06:04.085420Z",
     "shell.execute_reply.started": "2022-06-21T05:06:04.033208Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XTDNN(\n",
       "  (frame1): TDNN(\n",
       "    (kernel): Conv1d(40, 512, kernel_size=(5,), stride=(1,))\n",
       "    (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (frame2): TDNN(\n",
       "    (kernel): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "    (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (frame3): TDNN(\n",
       "    (kernel): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(3,))\n",
       "    (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (frame4): TDNN(\n",
       "    (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (frame5): TDNN(\n",
       "    (kernel): Conv1d(512, 1500, kernel_size=(1,), stride=(1,))\n",
       "    (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "    (bn): BatchNorm1d(1500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (tdnn_list): Sequential(\n",
       "    (0): TDNN(\n",
       "      (kernel): Conv1d(40, 512, kernel_size=(5,), stride=(1,))\n",
       "      (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TDNN(\n",
       "      (kernel): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "      (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TDNN(\n",
       "      (kernel): Conv1d(512, 512, kernel_size=(3,), stride=(1,), dilation=(3,))\n",
       "      (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TDNN(\n",
       "      (kernel): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TDNN(\n",
       "      (kernel): Conv1d(512, 1500, kernel_size=(1,), stride=(1,))\n",
       "      (nonlinearity): LeakyReLU(negative_slope=0.01)\n",
       "      (bn): BatchNorm1d(1500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (statspool): StatsPool()\n",
       "  (fc_embed): Linear(in_features=3000, out_features=512, bias=True)\n",
       "  (nl_embed): LeakyReLU(negative_slope=0.01)\n",
       "  (bn_embed): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop_embed): Dropout(p=0.0, inplace=False)\n",
       "  (soft_max): SoftMax()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load('./model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "best_acc = checkpoint['best_acc']\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:06:10.056712Z",
     "iopub.status.busy": "2022-06-21T05:06:10.056254Z",
     "iopub.status.idle": "2022-06-21T05:39:29.905262Z",
     "shell.execute_reply": "2022-06-21T05:39:29.904130Z",
     "shell.execute_reply.started": "2022-06-21T05:06:10.056674Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ... \n",
      "#################################################################\n",
      "=> Training Epoch #1, LR=0.0010000000\n",
      "| Epoch [ 1/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7125\n",
      "| Training loss 0.00039937\tTop1error 0.7125\n",
      "Epoch: 1/100, time so far: 0:00:18 (s), estimated time left: 0:30:42 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #2, LR=0.0010000000\n",
      "| Epoch [ 2/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7123\n",
      "| Training loss 0.00039966\tTop1error 0.7123\n",
      "Epoch: 2/100, time so far: 0:00:37 (s), estimated time left: 0:30:21 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #3, LR=0.0010000000\n",
      "| Epoch [ 3/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7083\n",
      "| Training loss 0.00040076\tTop1error 0.7083\n",
      "Epoch: 3/100, time so far: 0:00:56 (s), estimated time left: 0:30:16 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #4, LR=0.0010000000\n",
      "| Epoch [ 4/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7108\n",
      "| Training loss 0.00040020\tTop1error 0.7108\n",
      "Epoch: 4/100, time so far: 0:01:14 (s), estimated time left: 0:29:59 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #5, LR=0.0010000000\n",
      "| Epoch [ 5/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7134\n",
      "| Training loss 0.00039632\tTop1error 0.7134\n",
      "Epoch: 5/100, time so far: 0:01:33 (s), estimated time left: 0:29:34 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 65  41  52   5   4   3]\n",
      " [ 16 200  22   5   2   2]\n",
      " [ 35  21 154   3   1   5]\n",
      " [ 12   5   4 114  31  53]\n",
      " [  4   3   4  20 199  18]\n",
      " [  3   6   7  41  21 127]]\n",
      "acc_output: 0.6567278287461774, acc_prob: 0.6551987767584098\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 74  30  63   8   3   8]\n",
      " [ 17 236  15   2   4   1]\n",
      " [ 37  16 158   5   0   7]\n",
      " [ 13  11   9 173  31  65]\n",
      " [  3   5   4  20 206  23]\n",
      " [  4   5  11  51  10 125]]\n",
      "acc_output: 0.6689607708189952, acc_prob: 0.6641431520991053\n",
      "| Validation loss 0.00053708\tTop1acc 0.6614\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #6, LR=0.0010000000\n",
      "| Epoch [ 6/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7167\n",
      "| Training loss 0.00039758\tTop1error 0.7167\n",
      "Epoch: 6/100, time so far: 0:02:16 (s), estimated time left: 0:35:39 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #7, LR=0.0010000000\n",
      "| Epoch [ 7/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7132\n",
      "| Training loss 0.00039268\tTop1error 0.7132\n",
      "Epoch: 7/100, time so far: 0:02:35 (s), estimated time left: 0:34:27 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #8, LR=0.0010000000\n",
      "| Epoch [ 8/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7237\n",
      "| Training loss 0.00038794\tTop1error 0.7237\n",
      "Epoch: 8/100, time so far: 0:02:54 (s), estimated time left: 0:33:22 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #9, LR=0.0010000000\n",
      "| Epoch [ 9/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7163\n",
      "| Training loss 0.00038916\tTop1error 0.7163\n",
      "Epoch: 9/100, time so far: 0:03:12 (s), estimated time left: 0:32:24 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #10, LR=0.0010000000\n",
      "| Epoch [10/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7214\n",
      "| Training loss 0.00038603\tTop1error 0.7214\n",
      "Epoch: 10/100, time so far: 0:03:31 (s), estimated time left: 0:31:42 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 56  39  59   7   7   2]\n",
      " [ 12 201  25   4   3   2]\n",
      " [ 33  21 158   2   0   5]\n",
      " [  7   4  10 119  33  46]\n",
      " [  4   3   6  22 199  14]\n",
      " [  5   7   5  43  19 126]]\n",
      "acc_output: 0.6567278287461774, acc_prob: 0.6521406727828746\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 70  30  69   7   3   7]\n",
      " [ 15 238  16   1   3   2]\n",
      " [ 38  16 156   5   1   7]\n",
      " [ 12   8   7 177  34  64]\n",
      " [  3   5   6  23 205  19]\n",
      " [  4   4   9  47  10 132]]\n",
      "acc_output: 0.6730901582931865, acc_prob: 0.6772195457673779\n",
      "| Validation loss 0.00051816\tTop1acc 0.6621\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #11, LR=0.0010000000\n",
      "| Epoch [11/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7242\n",
      "| Training loss 0.00038210\tTop1error 0.7242\n",
      "Epoch: 11/100, time so far: 0:04:14 (s), estimated time left: 0:34:15 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #12, LR=0.0010000000\n",
      "| Epoch [12/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7274\n",
      "| Training loss 0.00038174\tTop1error 0.7274\n",
      "Epoch: 12/100, time so far: 0:04:32 (s), estimated time left: 0:33:18 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #13, LR=0.0010000000\n",
      "| Epoch [13/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7284\n",
      "| Training loss 0.00037924\tTop1error 0.7284\n",
      "Epoch: 13/100, time so far: 0:04:51 (s), estimated time left: 0:32:27 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #14, LR=0.0010000000\n",
      "| Epoch [14/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7278\n",
      "| Training loss 0.00037889\tTop1error 0.7278\n",
      "Epoch: 14/100, time so far: 0:05:09 (s), estimated time left: 0:31:43 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #15, LR=0.0010000000\n",
      "| Epoch [15/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7250\n",
      "| Training loss 0.00037958\tTop1error 0.7250\n",
      "Epoch: 15/100, time so far: 0:05:27 (s), estimated time left: 0:30:57 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 67  35  52   7   6   3]\n",
      " [ 17 198  25   3   1   3]\n",
      " [ 29  21 159   2   2   6]\n",
      " [  9   5   5 116  31  53]\n",
      " [  5   3   5  19 199  17]\n",
      " [  5   5   6  41  20 128]]\n",
      "acc_output: 0.6628440366972477, acc_prob: 0.6620795107033639\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 70  29  71   7   4   5]\n",
      " [ 18 235  15   2   4   1]\n",
      " [ 39  15 157   5   0   7]\n",
      " [  9   9   7 180  31  66]\n",
      " [  3   5   6  22 205  20]\n",
      " [  4   4   9  51  11 127]]\n",
      "acc_output: 0.6703372333103923, acc_prob: 0.6689607708189952\n",
      "| Validation loss 0.00052217\tTop1acc 0.6641\n",
      "#################################################################\n",
      "=> Training Epoch #16, LR=0.0010000000\n",
      "| Epoch [16/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7307\n",
      "| Training loss 0.00037693\tTop1error 0.7307\n",
      "Epoch: 16/100, time so far: 0:06:10 (s), estimated time left: 0:32:27 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #17, LR=0.0010000000\n",
      "| Epoch [17/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7314\n",
      "| Training loss 0.00037435\tTop1error 0.7314\n",
      "Epoch: 17/100, time so far: 0:06:29 (s), estimated time left: 0:31:39 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #18, LR=0.0010000000\n",
      "| Epoch [18/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7360\n",
      "| Training loss 0.00037230\tTop1error 0.7360\n",
      "Epoch: 18/100, time so far: 0:06:47 (s), estimated time left: 0:30:56 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #19, LR=0.0010000000\n",
      "| Epoch [19/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7366\n",
      "| Training loss 0.00037022\tTop1error 0.7366\n",
      "Epoch: 19/100, time so far: 0:07:05 (s), estimated time left: 0:30:13 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #20, LR=0.0010000000\n",
      "| Epoch [20/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7382\n",
      "| Training loss 0.00036689\tTop1error 0.7382\n",
      "Epoch: 20/100, time so far: 0:07:24 (s), estimated time left: 0:29:37 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 66  38  50   7   6   3]\n",
      " [ 16 199  23   4   2   3]\n",
      " [ 27  20 162   2   1   7]\n",
      " [ 10   5   8 118  30  48]\n",
      " [  5   3   6  19 199  16]\n",
      " [  5   3   6  41  23 127]]\n",
      "acc_output: 0.6659021406727829, acc_prob: 0.6651376146788991\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 75  31  64   7   3   6]\n",
      " [ 19 238  12   0   4   2]\n",
      " [ 38  15 159   4   0   7]\n",
      " [  9   9   8 177  34  65]\n",
      " [  3   5   5  25 204  19]\n",
      " [  4   4   9  50  10 129]]\n",
      "acc_output: 0.6758430832759807, acc_prob: 0.6758430832759807\n",
      "| Validation loss 0.00048517\tTop1acc 0.6793\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #21, LR=0.0010000000\n",
      "| Epoch [21/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7381\n",
      "| Training loss 0.00036952\tTop1error 0.7381\n",
      "Epoch: 21/100, time so far: 0:08:07 (s), estimated time left: 0:30:32 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #22, LR=0.0010000000\n",
      "| Epoch [22/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7395\n",
      "| Training loss 0.00036574\tTop1error 0.7395\n",
      "Epoch: 22/100, time so far: 0:08:25 (s), estimated time left: 0:29:52 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #23, LR=0.0010000000\n",
      "| Epoch [23/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7378\n",
      "| Training loss 0.00036232\tTop1error 0.7378\n",
      "Epoch: 23/100, time so far: 0:08:43 (s), estimated time left: 0:29:13 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #24, LR=0.0010000000\n",
      "| Epoch [24/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7409\n",
      "| Training loss 0.00036369\tTop1error 0.7409\n",
      "Epoch: 24/100, time so far: 0:09:02 (s), estimated time left: 0:28:38 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #25, LR=0.0010000000\n",
      "| Epoch [25/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7423\n",
      "| Training loss 0.00036363\tTop1error 0.7423\n",
      "Epoch: 25/100, time so far: 0:09:20 (s), estimated time left: 0:28:01 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 67  34  52   6   5   6]\n",
      " [ 15 200  23   5   1   3]\n",
      " [ 26  20 164   3   0   6]\n",
      " [  9   5  10 123  29  43]\n",
      " [  5   2   5  21 197  18]\n",
      " [  5   4   6  43  18 129]]\n",
      "acc_output: 0.672782874617737, acc_prob: 0.6704892966360856\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 71  33  65   9   3   5]\n",
      " [ 21 234  14   2   3   1]\n",
      " [ 36  15 160   5   0   7]\n",
      " [  9   8   8 182  29  66]\n",
      " [  3   6   5  20 206  21]\n",
      " [  3   4  11  54  11 123]]\n",
      "acc_output: 0.6717136958017894, acc_prob: 0.6758430832759807\n",
      "| Validation loss 0.00048598\tTop1acc 0.6683\n",
      "#################################################################\n",
      "=> Training Epoch #26, LR=0.0010000000\n",
      "| Epoch [26/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7427\n",
      "| Training loss 0.00035934\tTop1error 0.7427\n",
      "Epoch: 26/100, time so far: 0:10:03 (s), estimated time left: 0:28:37 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #27, LR=0.0010000000\n",
      "| Epoch [27/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7465\n",
      "| Training loss 0.00035843\tTop1error 0.7465\n",
      "Epoch: 27/100, time so far: 0:10:21 (s), estimated time left: 0:28:00 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #28, LR=0.0010000000\n",
      "| Epoch [28/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7434\n",
      "| Training loss 0.00035801\tTop1error 0.7434\n",
      "Epoch: 28/100, time so far: 0:10:40 (s), estimated time left: 0:27:26 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #29, LR=0.0010000000\n",
      "| Epoch [29/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7525\n",
      "| Training loss 0.00035584\tTop1error 0.7525\n",
      "Epoch: 29/100, time so far: 0:10:58 (s), estimated time left: 0:26:51 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #30, LR=0.0010000000\n",
      "| Epoch [30/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7507\n",
      "| Training loss 0.00035340\tTop1error 0.7507\n",
      "Epoch: 30/100, time so far: 0:11:16 (s), estimated time left: 0:26:19 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 63  37  51   7   7   5]\n",
      " [ 18 200  20   4   2   3]\n",
      " [ 31  17 163   2   1   5]\n",
      " [ 10   5   5 123  32  44]\n",
      " [  4   2   5  21 199  17]\n",
      " [  5   5   6  35  20 134]]\n",
      "acc_output: 0.6743119266055045, acc_prob: 0.6735474006116208\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 74  29  67   9   3   4]\n",
      " [ 16 240  14   0   4   1]\n",
      " [ 33  15 162   5   1   7]\n",
      " [ 10   8   7 181  32  64]\n",
      " [  3   5   4  23 207  19]\n",
      " [  6   4   7  50  10 129]]\n",
      "acc_output: 0.6834136269786648, acc_prob: 0.6799724707501721\n",
      "| Validation loss 0.00048416\tTop1acc 0.6669\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #31, LR=0.0010000000\n",
      "| Epoch [31/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7497\n",
      "| Training loss 0.00035376\tTop1error 0.7497\n",
      "Epoch: 31/100, time so far: 0:12:01 (s), estimated time left: 0:26:46 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #32, LR=0.0010000000\n",
      "| Epoch [32/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7537\n",
      "| Training loss 0.00035125\tTop1error 0.7537\n",
      "Epoch: 32/100, time so far: 0:12:20 (s), estimated time left: 0:26:13 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #33, LR=0.0010000000\n",
      "| Epoch [33/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7517\n",
      "| Training loss 0.00035159\tTop1error 0.7517\n",
      "Epoch: 33/100, time so far: 0:12:38 (s), estimated time left: 0:25:40 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #34, LR=0.0010000000\n",
      "| Epoch [34/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7557\n",
      "| Training loss 0.00034875\tTop1error 0.7557\n",
      "Epoch: 34/100, time so far: 0:12:56 (s), estimated time left: 0:25:07 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #35, LR=0.0010000000\n",
      "| Epoch [35/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7543\n",
      "| Training loss 0.00034998\tTop1error 0.7543\n",
      "Epoch: 35/100, time so far: 0:13:15 (s), estimated time left: 0:24:37 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 65  36  54   7   4   4]\n",
      " [ 15 200  24   5   1   2]\n",
      " [ 28  16 165   3   1   6]\n",
      " [  6   7   7 120  29  50]\n",
      " [  5   3   6  19 198  17]\n",
      " [  4   3   6  41  19 132]]\n",
      "acc_output: 0.672782874617737, acc_prob: 0.6735474006116208\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 79  27  61  11   3   5]\n",
      " [ 16 240  14   1   4   0]\n",
      " [ 35  13 165   4   0   6]\n",
      " [  8   9   7 184  28  66]\n",
      " [  3   4   4  24 207  19]\n",
      " [  5   4   7  45  13 132]]\n",
      "acc_output: 0.6930488644184446, acc_prob: 0.692360633172746\n",
      "| Validation loss 0.00047841\tTop1acc 0.6724\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #36, LR=0.0010000000\n",
      "| Epoch [36/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7553\n",
      "| Training loss 0.00034862\tTop1error 0.7553\n",
      "Epoch: 36/100, time so far: 0:13:58 (s), estimated time left: 0:24:50 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #37, LR=0.0010000000\n",
      "| Epoch [37/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7560\n",
      "| Training loss 0.00034654\tTop1error 0.7560\n",
      "Epoch: 37/100, time so far: 0:14:17 (s), estimated time left: 0:24:20 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #38, LR=0.0010000000\n",
      "| Epoch [38/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7581\n",
      "| Training loss 0.00034183\tTop1error 0.7581\n",
      "Epoch: 38/100, time so far: 0:14:36 (s), estimated time left: 0:23:49 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #39, LR=0.0010000000\n",
      "| Epoch [39/100] Iter [  6/  5]\tBatch loss 0.0004\tTop1acc 0.7575\n",
      "| Training loss 0.00034622\tTop1error 0.7575\n",
      "Epoch: 39/100, time so far: 0:14:55 (s), estimated time left: 0:23:20 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #40, LR=0.0010000000\n",
      "| Epoch [40/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7606\n",
      "| Training loss 0.00034094\tTop1error 0.7606\n",
      "Epoch: 40/100, time so far: 0:15:13 (s), estimated time left: 0:22:49 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 63  35  55   7   4   6]\n",
      " [ 17 202  19   7   1   1]\n",
      " [ 30  17 166   1   1   4]\n",
      " [ 10   4   6 115  32  52]\n",
      " [  3   3   6  23 199  14]\n",
      " [  4   3   5  40  19 134]]\n",
      "acc_output: 0.6720183486238532, acc_prob: 0.6704892966360856\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 77  27  66   9   3   4]\n",
      " [ 16 238  13   4   4   0]\n",
      " [ 34  12 164   5   1   7]\n",
      " [  9   8   7 181  33  64]\n",
      " [  3   5   5  22 209  17]\n",
      " [  4   4   7  48  10 133]]\n",
      "acc_output: 0.6896077081899519, acc_prob: 0.6868547832071576\n",
      "| Validation loss 0.00045362\tTop1acc 0.6813\n",
      "#################################################################\n",
      "=> Training Epoch #41, LR=0.0010000000\n",
      "| Epoch [41/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7607\n",
      "| Training loss 0.00034131\tTop1error 0.7607\n",
      "Epoch: 41/100, time so far: 0:15:55 (s), estimated time left: 0:22:55 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #42, LR=0.0010000000\n",
      "| Epoch [42/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7670\n",
      "| Training loss 0.00033673\tTop1error 0.7670\n",
      "Epoch: 42/100, time so far: 0:16:14 (s), estimated time left: 0:22:26 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #43, LR=0.0010000000\n",
      "| Epoch [43/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7664\n",
      "| Training loss 0.00033593\tTop1error 0.7664\n",
      "Epoch: 43/100, time so far: 0:16:33 (s), estimated time left: 0:21:56 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #44, LR=0.0010000000\n",
      "| Epoch [44/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7680\n",
      "| Training loss 0.00033451\tTop1error 0.7680\n",
      "Epoch: 44/100, time so far: 0:16:51 (s), estimated time left: 0:21:26 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #45, LR=0.0010000000\n",
      "| Epoch [45/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7688\n",
      "| Training loss 0.00033585\tTop1error 0.7688\n",
      "Epoch: 45/100, time so far: 0:17:09 (s), estimated time left: 0:20:58 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 66  32  59   7   2   4]\n",
      " [ 14 200  24   6   2   1]\n",
      " [ 32  15 162   3   2   5]\n",
      " [ 11   5   4 122  31  46]\n",
      " [  4   3   7  22 196  16]\n",
      " [  4   3   5  40  18 135]]\n",
      "acc_output: 0.6735474006116208, acc_prob: 0.6750764525993884\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 81  30  60   8   3   4]\n",
      " [ 17 237  15   2   4   0]\n",
      " [ 35  15 161   4   1   7]\n",
      " [  9   8   7 185  29  64]\n",
      " [  3   5   5  22 207  19]\n",
      " [  3   4   7  48  12 132]]\n",
      "acc_output: 0.6902959394356504, acc_prob: 0.6841018582243634\n",
      "| Validation loss 0.00046492\tTop1acc 0.6779\n",
      "#################################################################\n",
      "=> Training Epoch #46, LR=0.0010000000\n",
      "| Epoch [46/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7654\n",
      "| Training loss 0.00033401\tTop1error 0.7654\n",
      "Epoch: 46/100, time so far: 0:17:52 (s), estimated time left: 0:20:58 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #47, LR=0.0010000000\n",
      "| Epoch [47/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7699\n",
      "| Training loss 0.00033116\tTop1error 0.7699\n",
      "Epoch: 47/100, time so far: 0:18:11 (s), estimated time left: 0:20:30 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #48, LR=0.0010000000\n",
      "| Epoch [48/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7701\n",
      "| Training loss 0.00033113\tTop1error 0.7701\n",
      "Epoch: 48/100, time so far: 0:18:29 (s), estimated time left: 0:20:01 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #49, LR=0.0010000000\n",
      "| Epoch [49/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7696\n",
      "| Training loss 0.00032948\tTop1error 0.7696\n",
      "Epoch: 49/100, time so far: 0:18:47 (s), estimated time left: 0:19:33 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #50, LR=0.0010000000\n",
      "| Epoch [50/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7706\n",
      "| Training loss 0.00032994\tTop1error 0.7706\n",
      "Epoch: 50/100, time so far: 0:19:05 (s), estimated time left: 0:19:05 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 66  34  52  10   4   4]\n",
      " [ 18 199  22   5   1   2]\n",
      " [ 29  15 168   1   1   5]\n",
      " [ 12   5   7 122  26  47]\n",
      " [  2   2   6  21 198  19]\n",
      " [  5   4   5  40  20 131]]\n",
      "acc_output: 0.6758409785932722, acc_prob: 0.6743119266055045\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 77  29  63  10   3   4]\n",
      " [ 22 235  13   1   4   0]\n",
      " [ 35  12 168   3   0   5]\n",
      " [  8   8   8 185  30  63]\n",
      " [  3   5   6  23 207  17]\n",
      " [  3   3   9  46  12 133]]\n",
      "acc_output: 0.6916724019270475, acc_prob: 0.6951135581555402\n",
      "| Validation loss 0.00046442\tTop1acc 0.6834\n",
      "#################################################################\n",
      "=> Training Epoch #51, LR=0.0010000000\n",
      "| Epoch [51/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7736\n",
      "| Training loss 0.00032669\tTop1error 0.7736\n",
      "Epoch: 51/100, time so far: 0:19:48 (s), estimated time left: 0:19:01 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #52, LR=0.0010000000\n",
      "| Epoch [52/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7730\n",
      "| Training loss 0.00032559\tTop1error 0.7730\n",
      "Epoch: 52/100, time so far: 0:20:06 (s), estimated time left: 0:18:33 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #53, LR=0.0010000000\n",
      "| Epoch [53/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7761\n",
      "| Training loss 0.00032484\tTop1error 0.7761\n",
      "Epoch: 53/100, time so far: 0:20:25 (s), estimated time left: 0:18:06 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #54, LR=0.0010000000\n",
      "| Epoch [54/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7769\n",
      "| Training loss 0.00032480\tTop1error 0.7769\n",
      "Epoch: 54/100, time so far: 0:20:43 (s), estimated time left: 0:17:39 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #55, LR=0.0010000000\n",
      "| Epoch [55/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7758\n",
      "| Training loss 0.00032265\tTop1error 0.7758\n",
      "Epoch: 55/100, time so far: 0:21:02 (s), estimated time left: 0:17:12 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 66  33  54   8   5   4]\n",
      " [ 15 203  20   5   1   3]\n",
      " [ 34  21 157   2   1   4]\n",
      " [  9   6   6 122  29  47]\n",
      " [  5   4   5  18 199  17]\n",
      " [  5   5   5  37  19 134]]\n",
      "acc_output: 0.6735474006116208, acc_prob: 0.6704892966360856\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 83  30  57   7   3   6]\n",
      " [ 21 237  12   2   3   0]\n",
      " [ 32  13 166   4   1   7]\n",
      " [ 10   8   7 182  26  69]\n",
      " [  3   4   4  24 210  16]\n",
      " [  4   4   6  44  11 137]]\n",
      "acc_output: 0.698554714384033, acc_prob: 0.6930488644184446\n",
      "| Validation loss 0.00042592\tTop1acc 0.6800\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #56, LR=0.0010000000\n",
      "| Epoch [56/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7772\n",
      "| Training loss 0.00032094\tTop1error 0.7772\n",
      "Epoch: 56/100, time so far: 0:21:44 (s), estimated time left: 0:17:04 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #57, LR=0.0010000000\n",
      "| Epoch [57/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7772\n",
      "| Training loss 0.00031912\tTop1error 0.7772\n",
      "Epoch: 57/100, time so far: 0:22:03 (s), estimated time left: 0:16:38 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #58, LR=0.0010000000\n",
      "| Epoch [58/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7795\n",
      "| Training loss 0.00031759\tTop1error 0.7795\n",
      "Epoch: 58/100, time so far: 0:22:21 (s), estimated time left: 0:16:11 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #59, LR=0.0010000000\n",
      "| Epoch [59/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7830\n",
      "| Training loss 0.00031809\tTop1error 0.7830\n",
      "Epoch: 59/100, time so far: 0:22:40 (s), estimated time left: 0:15:45 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #60, LR=0.0010000000\n",
      "| Epoch [60/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7805\n",
      "| Training loss 0.00031786\tTop1error 0.7805\n",
      "Epoch: 60/100, time so far: 0:22:58 (s), estimated time left: 0:15:18 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 74  32  51   8   2   3]\n",
      " [ 19 201  20   3   2   2]\n",
      " [ 34  17 160   1   0   7]\n",
      " [  9   6   6 125  30  43]\n",
      " [  3   3   3  21 202  16]\n",
      " [  5   3   5  39  18 135]]\n",
      "acc_output: 0.6857798165137615, acc_prob: 0.6834862385321101\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 81  31  59   7   4   4]\n",
      " [ 17 239  14   1   3   1]\n",
      " [ 34  12 166   5   0   6]\n",
      " [  7   7   6 191  26  65]\n",
      " [  3   4   5  22 212  15]\n",
      " [  5   4   5  47  11 134]]\n",
      "acc_output: 0.7040605643496215, acc_prob: 0.7061252580867171\n",
      "| Validation loss 0.00044909\tTop1acc 0.6875\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #61, LR=0.0010000000\n",
      "| Epoch [61/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7842\n",
      "| Training loss 0.00031437\tTop1error 0.7842\n",
      "Epoch: 61/100, time so far: 0:23:42 (s), estimated time left: 0:15:09 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #62, LR=0.0010000000\n",
      "| Epoch [62/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7827\n",
      "| Training loss 0.00031637\tTop1error 0.7827\n",
      "Epoch: 62/100, time so far: 0:24:00 (s), estimated time left: 0:14:43 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #63, LR=0.0010000000\n",
      "| Epoch [63/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7858\n",
      "| Training loss 0.00031232\tTop1error 0.7858\n",
      "Epoch: 63/100, time so far: 0:24:19 (s), estimated time left: 0:14:17 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #64, LR=0.0010000000\n",
      "| Epoch [64/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7859\n",
      "| Training loss 0.00031081\tTop1error 0.7859\n",
      "Epoch: 64/100, time so far: 0:24:38 (s), estimated time left: 0:13:51 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #65, LR=0.0010000000\n",
      "| Epoch [65/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7835\n",
      "| Training loss 0.00031329\tTop1error 0.7835\n",
      "Epoch: 65/100, time so far: 0:24:56 (s), estimated time left: 0:13:25 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 65  33  54   9   4   5]\n",
      " [ 15 200  24   6   1   1]\n",
      " [ 30  16 166   1   2   4]\n",
      " [  9   4   6 130  27  43]\n",
      " [  2   3   5  23 199  16]\n",
      " [  5   4   4  37  18 137]]\n",
      "acc_output: 0.6857798165137615, acc_prob: 0.6865443425076453\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 80  28  63   7   3   5]\n",
      " [ 18 239  12   1   4   1]\n",
      " [ 36  12 163   4   0   8]\n",
      " [  6   7   7 199  26  57]\n",
      " [  3   4   4  21 212  17]\n",
      " [  3   3   8  44  13 135]]\n",
      "acc_output: 0.7075017205781142, acc_prob: 0.7088781830695113\n",
      "| Validation loss 0.00042058\tTop1acc 0.6930\n",
      "Saving model\n",
      "=======================================================================\n",
      "model saved\n",
      "#################################################################\n",
      "=> Training Epoch #66, LR=0.0010000000\n",
      "| Epoch [66/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7907\n",
      "| Training loss 0.00030938\tTop1error 0.7907\n",
      "Epoch: 66/100, time so far: 0:25:39 (s), estimated time left: 0:13:13 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #67, LR=0.0010000000\n",
      "| Epoch [67/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7901\n",
      "| Training loss 0.00030728\tTop1error 0.7901\n",
      "Epoch: 67/100, time so far: 0:25:58 (s), estimated time left: 0:12:47 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #68, LR=0.0010000000\n",
      "| Epoch [68/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7870\n",
      "| Training loss 0.00030789\tTop1error 0.7870\n",
      "Epoch: 68/100, time so far: 0:26:16 (s), estimated time left: 0:12:21 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #69, LR=0.0010000000\n",
      "| Epoch [69/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7918\n",
      "| Training loss 0.00030706\tTop1error 0.7918\n",
      "Epoch: 69/100, time so far: 0:26:35 (s), estimated time left: 0:11:56 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #70, LR=0.0010000000\n",
      "| Epoch [70/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7946\n",
      "| Training loss 0.00030561\tTop1error 0.7946\n",
      "Epoch: 70/100, time so far: 0:26:53 (s), estimated time left: 0:11:31 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 65  32  56   7   4   6]\n",
      " [ 14 202  24   4   1   2]\n",
      " [ 31  15 166   1   1   5]\n",
      " [  8   6   7 123  25  50]\n",
      " [  4   2   4  20 203  15]\n",
      " [  5   4   5  39  17 135]]\n",
      "acc_output: 0.6834862385321101, acc_prob: 0.6819571865443425\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 81  28  61   9   2   5]\n",
      " [ 15 238  15   2   4   1]\n",
      " [ 35  10 169   4   0   5]\n",
      " [  5   8   9 186  29  65]\n",
      " [  3   5   6  22 208  17]\n",
      " [  4   3   7  45   9 138]]\n",
      "acc_output: 0.7019958706125258, acc_prob: 0.7006194081211287\n",
      "| Validation loss 0.00042933\tTop1acc 0.6910\n",
      "#################################################################\n",
      "=> Training Epoch #71, LR=0.0010000000\n",
      "| Epoch [71/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7941\n",
      "| Training loss 0.00030383\tTop1error 0.7941\n",
      "Epoch: 71/100, time so far: 0:27:35 (s), estimated time left: 0:11:16 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #72, LR=0.0010000000\n",
      "| Epoch [72/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7911\n",
      "| Training loss 0.00030426\tTop1error 0.7911\n",
      "Epoch: 72/100, time so far: 0:27:54 (s), estimated time left: 0:10:51 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #73, LR=0.0010000000\n",
      "| Epoch [73/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7947\n",
      "| Training loss 0.00030144\tTop1error 0.7947\n",
      "Epoch: 73/100, time so far: 0:28:13 (s), estimated time left: 0:10:26 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #74, LR=0.0010000000\n",
      "| Epoch [74/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7967\n",
      "| Training loss 0.00030076\tTop1error 0.7967\n",
      "Epoch: 74/100, time so far: 0:28:31 (s), estimated time left: 0:10:01 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #75, LR=0.0010000000\n",
      "| Epoch [75/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7978\n",
      "| Training loss 0.00029965\tTop1error 0.7978\n",
      "Epoch: 75/100, time so far: 0:28:50 (s), estimated time left: 0:09:36 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 65  33  56   9   4   3]\n",
      " [ 21 196  21   4   2   3]\n",
      " [ 29  17 166   1   1   5]\n",
      " [  8   7   7 128  27  42]\n",
      " [  2   2   4  21 204  15]\n",
      " [  5   3   5  33  18 141]]\n",
      "acc_output: 0.6880733944954128, acc_prob: 0.6880733944954128\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 83  26  62   9   3   3]\n",
      " [ 18 238  14   1   4   0]\n",
      " [ 34  12 164   5   1   7]\n",
      " [  7   6   7 188  27  67]\n",
      " [  3   5   4  21 211  17]\n",
      " [  5   3   7  49  11 131]]\n",
      "acc_output: 0.698554714384033, acc_prob: 0.7013076393668273\n",
      "| Validation loss 0.00039929\tTop1acc 0.6937\n",
      "#################################################################\n",
      "=> Training Epoch #76, LR=0.0010000000\n",
      "| Epoch [76/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8007\n",
      "| Training loss 0.00029786\tTop1error 0.8007\n",
      "Epoch: 76/100, time so far: 0:29:33 (s), estimated time left: 0:09:20 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #77, LR=0.0010000000\n",
      "| Epoch [77/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.7983\n",
      "| Training loss 0.00029909\tTop1error 0.7983\n",
      "Epoch: 77/100, time so far: 0:29:52 (s), estimated time left: 0:08:55 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #78, LR=0.0010000000\n",
      "| Epoch [78/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8018\n",
      "| Training loss 0.00029536\tTop1error 0.8018\n",
      "Epoch: 78/100, time so far: 0:30:15 (s), estimated time left: 0:08:32 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #79, LR=0.0010000000\n",
      "| Epoch [79/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8030\n",
      "| Training loss 0.00029499\tTop1error 0.8030\n",
      "Epoch: 79/100, time so far: 0:30:33 (s), estimated time left: 0:08:07 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #80, LR=0.0010000000\n",
      "| Epoch [80/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8027\n",
      "| Training loss 0.00029379\tTop1error 0.8027\n",
      "Epoch: 80/100, time so far: 0:30:53 (s), estimated time left: 0:07:43 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 70  33  51   9   3   4]\n",
      " [ 18 201  20   4   2   2]\n",
      " [ 30  15 166   1   1   6]\n",
      " [  8   6   4 128  24  49]\n",
      " [  4   3   3  23 200  15]\n",
      " [  4   5   5  34  17 140]]\n",
      "acc_output: 0.6918960244648318, acc_prob: 0.6918960244648318\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 82  28  62   7   3   4]\n",
      " [ 18 237  14   3   3   0]\n",
      " [ 36  13 163   5   0   6]\n",
      " [  6   7   8 197  25  59]\n",
      " [  3   5   5  21 211  16]\n",
      " [  2   3   7  45  11 138]]\n",
      "acc_output: 0.7075017205781142, acc_prob: 0.7116311080523056\n",
      "| Validation loss 0.00041220\tTop1acc 0.7068\n",
      "#################################################################\n",
      "=> Training Epoch #81, LR=0.0010000000\n",
      "| Epoch [81/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8010\n",
      "| Training loss 0.00029579\tTop1error 0.8010\n",
      "Epoch: 81/100, time so far: 0:31:35 (s), estimated time left: 0:07:24 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #82, LR=0.0010000000\n",
      "| Epoch [82/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8040\n",
      "| Training loss 0.00029278\tTop1error 0.8040\n",
      "Epoch: 82/100, time so far: 0:31:54 (s), estimated time left: 0:07:00 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #83, LR=0.0010000000\n",
      "| Epoch [83/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8064\n",
      "| Training loss 0.00029031\tTop1error 0.8064\n",
      "Epoch: 83/100, time so far: 0:32:12 (s), estimated time left: 0:06:35 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #84, LR=0.0010000000\n",
      "| Epoch [84/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8032\n",
      "| Training loss 0.00029170\tTop1error 0.8032\n",
      "Epoch: 84/100, time so far: 0:32:31 (s), estimated time left: 0:06:11 (s)\n",
      "#################################################################\n",
      "=> Training Epoch #85, LR=0.0010000000\n",
      "| Epoch [85/100] Iter [  6/  5]\tBatch loss 0.0003\tTop1acc 0.8053\n",
      "| Training loss 0.00029058\tTop1error 0.8053\n",
      "Epoch: 85/100, time so far: 0:32:48 (s), estimated time left: 0:05:47 (s)\n",
      "On test set\n",
      "test 1/2\n",
      "test 2/2\n",
      "[[ 72  31  53   7   3   4]\n",
      " [ 15 201  23   3   2   3]\n",
      " [ 26  17 170   0   1   5]\n",
      " [  8   6   7 130  25  43]\n",
      " [  2   3   4  21 202  16]\n",
      " [  4   4   5  39  17 136]]\n",
      "acc_output: 0.6964831804281345, acc_prob: 0.6934250764525994\n",
      "On validation\n",
      "test 1/3\n",
      "test 2/3\n",
      "test 3/3\n",
      "[[ 82  26  62   8   3   5]\n",
      " [ 17 240  12   1   4   1]\n",
      " [ 32  12 172   3   0   4]\n",
      " [  6   7   7 190  27  65]\n",
      " [  3   5   5  23 208  17]\n",
      " [  4   3   6  48  13 132]]\n",
      "acc_output: 0.70474879559532, acc_prob: 0.7068134893324157\n",
      "| Validation loss 0.00040460\tTop1acc 0.6979\n",
      "#################################################################\n",
      "=> Training Epoch #86, LR=0.0010000000\n",
      "| Epoch [86/100] Iter [  1/  5]\tBatch loss 0.0003\tTop1acc 0.7930\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_33/4168434284.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0mrunning_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0mrunning_corrects\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mpreds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0mtot\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "########## Start training\n",
    "print('Start training ... ')\n",
    "t0 = time()\n",
    "for epoch in range(args.num_epochs):\n",
    "\n",
    "    print('#################################################################')\n",
    "    print('=> Training Epoch #%d, LR=%.10f' % (epoch + 1, lr))\n",
    "    running_loss, running_corrects, tot = 0.0, 0.0, 0.0\n",
    "    running_loss_src, running_corrects_src, tot_src = 0.0, 0.0, 0.0\n",
    "    ########################\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    ## Training\n",
    "    for batch_idx, (inputs, labels, _) in enumerate(dset_loaders['train']):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        a, preds = torch.max(outputs.data, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += preds.eq(labels.data).cpu().sum()\n",
    "        tot += labels.size(0)\n",
    "        sys.stdout.write('\\r')\n",
    "        try:\n",
    "            batch_loss = loss.item()\n",
    "        except NameError:\n",
    "            batch_loss = 0\n",
    "\n",
    "        top1acc = float(running_corrects) / tot\n",
    "        sys.stdout.write('| Epoch [%2d/%2d] Iter [%3d/%3d]\\tBatch loss %.4f\\tTop1acc %.4f'\n",
    "                         % (epoch + 1, args.num_epochs, batch_idx + 1,\n",
    "                            (len(train_fns) // args.batch_size), batch_loss / args.batch_size,\n",
    "                            top1acc))\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write('\\r')\n",
    "\n",
    "    top1acc = float(running_corrects) / N_train\n",
    "    epoch_loss = running_loss / N_train\n",
    "    print('\\n| Training loss %.8f\\tTop1error %.4f' \\\n",
    "          % (epoch_loss, top1acc))\n",
    "\n",
    "    print_eta(t0, epoch, args.num_epochs)\n",
    "    ###################################\n",
    "    ## Validation\n",
    "    if (epoch + 1) % args.check_after == 0:\n",
    "        # Validation\n",
    "        ######################\n",
    "        n_files = len(val_lbs)\n",
    "        print('On test set')\n",
    "        pred_output, pred_prob, _ = singlemodel_class(model, dset_loaders['test'], num_tests=2)\n",
    "        print(confusion_matrix(semi_lbs, pred_output))\n",
    "        acc1 = accuracy_score(semi_lbs, pred_output)\n",
    "        acc2 = accuracy_score(semi_lbs, pred_prob)\n",
    "        print('acc_output: {}, acc_prob: {}'.format(acc1, acc2))\n",
    "        print('On validation')\n",
    "        pred_output, pred_prob, _ = singlemodel_class(model, dset_loaders['val'], num_tests=args.n_tests)\n",
    "        print(confusion_matrix(val_lbs, pred_output))\n",
    "        acc1 = accuracy_score(val_lbs, pred_output)\n",
    "        acc2 = accuracy_score(val_lbs, pred_prob)\n",
    "        print('acc_output: {}, acc_prob: {}'.format(acc1, acc2))\n",
    "        ########## end test on multiple windows ##############3\n",
    "        running_loss, running_corrects, tot = 0.0, 0.0, 0.0\n",
    "        torch.set_grad_enabled(False)\n",
    "        model.eval()\n",
    "        for batch_idx, (inputs, labels, _) in enumerate(dset_loaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += preds.eq(labels.data).cpu().sum()\n",
    "            tot += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / N_valid\n",
    "        top1acc = float(running_corrects) / N_valid\n",
    "        # top3error = 1 - float(runnning_topk_corrects)/N_valid\n",
    "        print('| Validation loss %.8f\\tTop1acc %.4f' \\\n",
    "              % (epoch_loss, top1acc))\n",
    "\n",
    "        ################## save model based on best acc\n",
    "        if acc1 > best_acc:\n",
    "            best_acc = acc1\n",
    "            print('Saving model')\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'best_acc': best_acc\n",
    "            }, 'model.pt')\n",
    "\n",
    "            print('=======================================================================')\n",
    "            print('model saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:39:36.196410Z",
     "iopub.status.busy": "2022-06-21T05:39:36.195910Z",
     "iopub.status.idle": "2022-06-21T05:39:52.614420Z",
     "shell.execute_reply": "2022-06-21T05:39:52.613415Z",
     "shell.execute_reply.started": "2022-06-21T05:39:36.196366Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1/5\n",
      "test 2/5\n",
      "test 3/5\n",
      "test 4/5\n",
      "test 5/5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "running_loss, running_corrects, tot = 0.0, 0.0, 0.0\n",
    "lbs = []\n",
    "pred1 = []\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "submit_pred,b,_ = singlemodel_class(model, dset_loaders['test'], num_tests = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T05:39:59.749385Z",
     "iopub.status.busy": "2022-06-21T05:39:59.748854Z",
     "iopub.status.idle": "2022-06-21T05:39:59.764898Z",
     "shell.execute_reply": "2022-06-21T05:39:59.763986Z",
     "shell.execute_reply.started": "2022-06-21T05:39:59.749344Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5583    0.3941    0.4621       170\n",
      "           1     0.7710    0.8178    0.7937       247\n",
      "           2     0.6554    0.7991    0.7202       219\n",
      "           3     0.6400    0.5845    0.6110       219\n",
      "           4     0.8039    0.8266    0.8151       248\n",
      "           5     0.6667    0.6634    0.6650       205\n",
      "\n",
      "    accuracy                         0.6980      1308\n",
      "   macro avg     0.6826    0.6809    0.6778      1308\n",
      "weighted avg     0.6920    0.6980    0.6916      1308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(semi_lbs,submit_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:07:04.424839Z",
     "iopub.status.busy": "2022-06-21T03:07:04.424552Z",
     "iopub.status.idle": "2022-06-21T03:07:07.620231Z",
     "shell.execute_reply": "2022-06-21T03:07:07.619459Z",
     "shell.execute_reply.started": "2022-06-21T03:07:04.424808Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submit_lbs,submit_fns = get_fn_submit()\n",
    "a = MyDatasetSTFT(submit_fns, submit_lbs, duration = args.duration)\n",
    "dset_loaders['submit'] = DataLoader(a,\n",
    "batch_size = 1,\n",
    "shuffle = False,\n",
    "num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:07:14.116858Z",
     "iopub.status.busy": "2022-06-21T03:07:14.116575Z",
     "iopub.status.idle": "2022-06-21T03:07:42.435862Z",
     "shell.execute_reply": "2022-06-21T03:07:42.434980Z",
     "shell.execute_reply.started": "2022-06-21T03:07:14.116827Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1/1\n"
     ]
    }
   ],
   "source": [
    "running_loss, running_corrects, tot = 0.0, 0.0, 0.0\n",
    "lbs = []\n",
    "pred1 = []\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "submit_pred,b,_ = singlemodel_class(model, dset_loaders['submit'], num_tests = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T03:07:52.046725Z",
     "iopub.status.busy": "2022-06-21T03:07:52.046446Z",
     "iopub.status.idle": "2022-06-21T03:07:52.065394Z",
     "shell.execute_reply": "2022-06-21T03:07:52.064545Z",
     "shell.execute_reply.started": "2022-06-21T03:07:52.046692Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9505    0.9683    0.9593      1705\n",
      "           1     0.9746    0.9602    0.9674      2161\n",
      "\n",
      "    accuracy                         0.9638      3866\n",
      "   macro avg     0.9626    0.9643    0.9633      3866\n",
      "weighted avg     0.9640    0.9638    0.9638      3866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(submit_lbs,submit_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}