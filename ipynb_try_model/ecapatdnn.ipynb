{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "NUM_WORKERS = 2\n",
    "BASE_TRAIN = '../input/gendersclassification-vdt-2022/dataset_v2/train'\n",
    "BASE_TEST = '../input/gendersclassification-vdt-2022/public-test/public-test/wav'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:09:29.505083Z",
     "iopub.execute_input": "2022-06-13T13:09:29.506112Z",
     "iopub.status.idle": "2022-06-13T13:09:29.533706Z",
     "shell.execute_reply.started": "2022-06-13T13:09:29.506005Z",
     "shell.execute_reply": "2022-06-13T13:09:29.532879Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "def get_fn_lbs():\n",
    "    lbs = []\n",
    "    fns = []\n",
    "    for name_dir in os.listdir(BASE_TRAIN):\n",
    "        for i in os.listdir(os.path.join(BASE_TRAIN,name_dir)):\n",
    "            if name_dir == 'male':\n",
    "                for j in os.listdir(os.path.join(BASE_TRAIN,name_dir,i)):\n",
    "                    dur = librosa.get_duration(filename=os.path.join(BASE_TRAIN,name_dir,i,j))\n",
    "                    if dur >1:\n",
    "                        lbs.append(1)\n",
    "                        fns.append(os.path.join(BASE_TRAIN,name_dir,i,j))\n",
    "            if name_dir == 'female':\n",
    "                for j in os.listdir(os.path.join(BASE_TRAIN,name_dir,i)):\n",
    "                    dur = librosa.get_duration(filename=os.path.join(BASE_TRAIN,name_dir,i,j))\n",
    "                    if dur >1:\n",
    "                        lbs.append(0)\n",
    "                        fns.append(os.path.join(BASE_TRAIN,name_dir,i,j))\n",
    "    return lbs,fns\n",
    "\n",
    "\n",
    "def get_fn_test():\n",
    "    lbs = []\n",
    "    fns = []\n",
    "    df = pd.read_csv(\"../input/gendersclassification-vdt-2022/public-test/public-test/test_files.txt\", header=None,names=[\"gender\", \"filename\"])\n",
    "    for index, row in df.iterrows():\n",
    "        if str(row['gender']) == 'M':\n",
    "            dur = librosa.get_duration(filename=os.path.join('../input/gendersclassification-vdt-2022/public-test/public-test/wav',str(row['filename'])))\n",
    "            if dur >1:\n",
    "                lbs.append(1)\n",
    "                fns.append(os.path.join('../input/gendersclassification-vdt-2022/public-test/public-test/wav',str(row['filename'])))\n",
    "        elif str(row['gender']) == 'F':\n",
    "            dur = librosa.get_duration(filename=os.path.join('../input/gendersclassification-vdt-2022/public-test/public-test/wav',str(row['filename'])))\n",
    "            if dur >1:\n",
    "                lbs.append(0)\n",
    "                fns.append(os.path.join('../input/gendersclassification-vdt-2022/public-test/public-test/wav',str(row['filename'])))\n",
    "    return lbs,fns\n",
    "test_lbs,test_fns = get_fn_test()\n",
    "\n",
    "def get_fn_submit():\n",
    "    fns = []\n",
    "    lbs = []\n",
    "    for i in os.listdir('../input/gendersclassification-vdt-2022/private-test/private-test/wav'):\n",
    "        fns.append(os.path.join('../input/gendersclassification-vdt-2022/private-test/private-test/wav',i))\n",
    "        lbs.append(0)\n",
    "    return lbs,fns\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:09:57.613403Z",
     "iopub.execute_input": "2022-06-13T13:09:57.614015Z",
     "iopub.status.idle": "2022-06-13T13:10:01.832813Z",
     "shell.execute_reply.started": "2022-06-13T13:09:57.613970Z",
     "shell.execute_reply": "2022-06-13T13:10:01.831915Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import librosa\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "\n",
    "def load_audio(audio_path, feature_method='melspectrogram', mode='train', sr=8000, chunk_duration=2, augmentors=None):\n",
    "\n",
    "    wav, sr_ret = librosa.load(audio_path, sr=sr)\n",
    "    if mode == 'train':\n",
    "\n",
    "        num_wav_samples = wav.shape[0]\n",
    "\n",
    "        if num_wav_samples < sr:\n",
    "            raise Exception(f'：{(num_wav_samples/sr):.2f}s')\n",
    "        num_chunk_samples = int(chunk_duration * sr)\n",
    "        if num_wav_samples > num_chunk_samples + 1:\n",
    "            start = random.randint(0, num_wav_samples - num_chunk_samples - 1)\n",
    "            stop = start + num_chunk_samples\n",
    "            wav = wav[start:stop]\n",
    "            if random.random() > 0.5:\n",
    "                wav[:random.randint(1, sr // 4)] = 0\n",
    "                wav = wav[:-random.randint(1, sr // 4)]\n",
    "\n",
    "        if augmentors is not None:\n",
    "            for key, augmentor in augmentors.items():\n",
    "                if key == 'specaug':continue\n",
    "                wav = augmentor(wav)\n",
    "    elif mode == 'eval':\n",
    "\n",
    "        num_wav_samples = wav.shape[0]\n",
    "        num_chunk_samples = int(chunk_duration * sr)\n",
    "        if num_wav_samples > num_chunk_samples + 1:\n",
    "            wav = wav[:num_chunk_samples]\n",
    "\n",
    "    if feature_method == 'melspectrogram':\n",
    "        features = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=400, n_mels=80, hop_length=160, win_length=400)\n",
    "    elif feature_method == 'spectrogram':\n",
    "        linear = librosa.stft(wav, n_fft=400, win_length=400, hop_length=160)\n",
    "        features, _ = librosa.magphase(linear)\n",
    "    else:\n",
    "        raise Exception(f' not have{feature_method} ！')\n",
    "    features = librosa.power_to_db(features, ref=1.0, amin=1e-10, top_db=None)\n",
    "    if mode == 'train' and augmentors is not None:\n",
    "        for key, augmentor in augmentors.items():\n",
    "            if key == 'specaug':\n",
    "                features = augmentor(features)\n",
    "    mean = np.mean(features, 0, keepdims=True)\n",
    "    std = np.std(features, 0, keepdims=True)\n",
    "    features = (features - mean) / (std + 1e-5)\n",
    "    return features\n",
    "\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, filenames,labels, feature_method='melspectrogram', mode='train', sr=8000, chunk_duration=2, augmentors=None):\n",
    "        super(CustomDataset, self).__init__()\n",
    "\n",
    "        self.feature_method = feature_method\n",
    "        self.mode = mode\n",
    "        self.sr = sr\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.augmentors = augmentors\n",
    "        self.fns = filenames\n",
    "        self.lbs = labels\n",
    "    def __getitem__(self, idx):\n",
    "            fname = self.fns[idx]\n",
    "            label = self.lbs[idx]\n",
    "            features = load_audio(fname, feature_method=self.feature_method, mode=self.mode, sr=self.sr,\n",
    "                                  chunk_duration=self.chunk_duration, augmentors=self.augmentors)\n",
    "            return features, np.array(int(label), dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    @property\n",
    "    def input_size(self):\n",
    "        if self.feature_method == 'melspectrogram':\n",
    "            return 80\n",
    "        elif self.feature_method == 'spectrogram':\n",
    "            return 201\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = sorted(batch, key=lambda sample: sample[0].shape[1], reverse=True)\n",
    "    freq_size = batch[0][0].shape[0]\n",
    "    max_audio_length = batch[0][0].shape[1]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    inputs = np.zeros((batch_size, freq_size, max_audio_length), dtype='float32')\n",
    "    input_lens = []\n",
    "    labels = []\n",
    "    for x in range(batch_size):\n",
    "        sample = batch[x]\n",
    "        tensor = sample[0]\n",
    "        labels.append(sample[1])\n",
    "        seq_length = tensor.shape[1]\n",
    "        inputs[x, :, :seq_length] = tensor[:, :]\n",
    "        input_lens.append(seq_length/max_audio_length)\n",
    "    input_lens = np.array(input_lens, dtype='float32')\n",
    "    labels = np.array(labels, dtype='int64')\n",
    "    return torch.tensor(inputs), torch.tensor(labels), torch.tensor(input_lens)\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-06-13T13:10:01.838035Z",
     "iopub.execute_input": "2022-06-13T13:10:01.838573Z",
     "iopub.status.idle": "2022-06-13T13:10:01.881529Z",
     "shell.execute_reply.started": "2022-06-13T13:10:01.838533Z",
     "shell.execute_reply": "2022-06-13T13:10:01.880555Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "class Res2Conv1dReluBn(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False, scale=4):\n",
    "        super().__init__()\n",
    "        assert channels % scale == 0, \"{} % {} != 0\".format(channels, scale)\n",
    "        self.scale = scale\n",
    "        self.width = channels // scale\n",
    "        self.nums = scale if scale == 1 else scale - 1\n",
    "\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(nn.Conv1d(self.width, self.width, kernel_size, stride, padding, dilation, bias=bias))\n",
    "            self.bns.append(nn.BatchNorm1d(self.width))\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.bns = nn.ModuleList(self.bns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        spx = torch.split(x, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            # Order: conv -> relu -> bn\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.bns[i](F.relu(sp))\n",
    "            out.append(sp)\n",
    "        if self.scale != 1:\n",
    "            out.append(spx[self.nums])\n",
    "        out = torch.cat(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv1dReluBn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(F.relu(self.conv(x)))\n",
    "\n",
    "\n",
    "class SE_Connect(nn.Module):\n",
    "    def __init__(self, channels, s=2):\n",
    "        super().__init__()\n",
    "        assert channels % s == 0, \"{} % {} != 0\".format(channels, s)\n",
    "        self.linear1 = nn.Linear(channels, channels // s)\n",
    "        self.linear2 = nn.Linear(channels // s, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.mean(dim=2)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        out = x * out.unsqueeze(2)\n",
    "        return out\n",
    "\n",
    "\n",
    "def SE_Res2Block(channels, kernel_size, stride, padding, dilation, scale):\n",
    "    return nn.Sequential(\n",
    "        Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        Res2Conv1dReluBn(channels, kernel_size, stride, padding, dilation, scale=scale),\n",
    "        Conv1dReluBn(channels, channels, kernel_size=1, stride=1, padding=0),\n",
    "        SE_Connect(channels)\n",
    "    )\n",
    "\n",
    "\n",
    "class AttentiveStatsPool(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
    "        self.linear1 = nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1)  # equals W and b in the paper\n",
    "        self.linear2 = nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1)  # equals V and k in the paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DON'T use ReLU here! In experiments, I find ReLU hard to converge.\n",
    "        alpha = torch.tanh(self.linear1(x))\n",
    "        alpha = torch.softmax(self.linear2(alpha), dim=2)\n",
    "        mean = torch.sum(alpha * x, dim=2)\n",
    "        residuals = torch.sum(alpha * x ** 2, dim=2) - mean ** 2\n",
    "        std = torch.sqrt(residuals.clamp(min=1e-9))\n",
    "        return torch.cat([mean, std], dim=1)\n",
    "\n",
    "\n",
    "class EcapaTdnn(nn.Module):\n",
    "    def __init__(self, input_size=80, channels=512, embd_dim=192):\n",
    "        super().__init__()\n",
    "        self.layer1 = Conv1dReluBn(input_size, channels, kernel_size=5, padding=2, dilation=1)\n",
    "        self.layer2 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=2, dilation=2, scale=8)\n",
    "        self.layer3 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=3, dilation=3, scale=8)\n",
    "        self.layer4 = SE_Res2Block(channels, kernel_size=3, stride=1, padding=4, dilation=4, scale=8)\n",
    "\n",
    "        cat_channels = channels * 3\n",
    "        out_channels = cat_channels * 2\n",
    "        self.emb_size = embd_dim\n",
    "        self.conv = nn.Conv1d(cat_channels, cat_channels, kernel_size=1)\n",
    "        self.pooling = AttentiveStatsPool(cat_channels, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.linear = nn.Linear(out_channels, embd_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(embd_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1) + out1\n",
    "        out3 = self.layer3(out1 + out2) + out1 + out2\n",
    "        out4 = self.layer4(out1 + out2 + out3) + out1 + out2 + out3\n",
    "\n",
    "        out = torch.cat([out2, out3, out4], dim=1)\n",
    "        out = F.relu(self.conv(out))\n",
    "        out = self.bn1(self.pooling(out))\n",
    "        out = self.bn2(self.linear(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Classification(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone,\n",
    "            num_class=1,\n",
    "            lin_blocks=0,\n",
    "            lin_neurons=192,\n",
    "            dropout=0.1, ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            backbone (Paddle.nn.Layer class): the speaker identification backbone network model\n",
    "            num_class (_type_): the speaker class num in the training dataset\n",
    "            lin_blocks (int, optional): the linear layer transform between the embedding and the final linear layer. Defaults to 0.\n",
    "            lin_neurons (int, optional): the output dimension of final linear layer. Defaults to 192.\n",
    "            dropout (float, optional): the dropout factor on the embedding. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super(Classification, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "\n",
    "        input_size = self.backbone.emb_size\n",
    "        self.blocks = list()\n",
    "        for i in range(lin_blocks):\n",
    "            self.blocks.extend([\n",
    "                nn.BatchNorm1d(input_size),\n",
    "                nn.Linear(in_features=input_size, out_features=lin_neurons),\n",
    "            ])\n",
    "            input_size = lin_neurons\n",
    "\n",
    "        # the final layer\n",
    "        self.weight = Parameter(torch.FloatTensor(num_class, input_size), requires_grad=True)\n",
    "        nn.init.xavier_normal_(self.weight, gain=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Do the speaker identification model forwrd,\n",
    "           including the speaker embedding model and the classifier model network\n",
    "\n",
    "        Args:\n",
    "            x (paddle.Tensor): input audio feats,\n",
    "                               shape=[batch, dimension, times]\n",
    "            lengths (paddle.Tensor, optional): input audio length.\n",
    "                                        shape=[batch, times]\n",
    "                                        Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            paddle.Tensor: return the logits of the feats\n",
    "        \"\"\"\n",
    "        # x.shape: (N, C, L)\n",
    "        x = self.backbone(x)  # (N, emb_size)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        for fc in self.blocks:\n",
    "            x = fc(x)\n",
    "\n",
    "        logits = F.linear(F.normalize(x), F.normalize(self.weight, dim=-1))\n",
    "\n",
    "        return logits\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:10:01.883014Z",
     "iopub.execute_input": "2022-06-13T13:10:01.883343Z",
     "iopub.status.idle": "2022-06-13T13:10:01.956351Z",
     "shell.execute_reply.started": "2022-06-13T13:10:01.883298Z",
     "shell.execute_reply": "2022-06-13T13:10:01.955353Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AdditiveAngularMargin(nn.Module):\n",
    "    def __init__(self, margin=0.0, scale=1.0, easy_margin=False):\n",
    "        \"\"\"The Implementation of Additive Angular Margin (AAM) proposed\n",
    "       in the following paper: '''Margin Matters: Towards More Discriminative Deep Neural Network Embeddings for Speaker Recognition'''\n",
    "       (https://arxiv.org/abs/1906.07317)\n",
    "\n",
    "        Args:\n",
    "            margin (float, optional): margin factor. Defaults to 0.0.\n",
    "            scale (float, optional): scale factor. Defaults to 1.0.\n",
    "            easy_margin (bool, optional): easy_margin flag. Defaults to False.\n",
    "        \"\"\"\n",
    "        super(AdditiveAngularMargin, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        self.easy_margin = easy_margin\n",
    "\n",
    "        self.cos_m = math.cos(self.margin)\n",
    "        self.sin_m = math.sin(self.margin)\n",
    "        self.th = math.cos(math.pi - self.margin)\n",
    "        self.mm = math.sin(math.pi - self.margin) * self.margin\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        cosine = outputs.float()\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        outputs = (targets * phi) + ((1.0 - targets) * cosine)\n",
    "        return self.scale * outputs\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:10:01.961562Z",
     "iopub.execute_input": "2022-06-13T13:10:01.962331Z",
     "iopub.status.idle": "2022-06-13T13:10:01.979614Z",
     "shell.execute_reply.started": "2022-06-13T13:10:01.962275Z",
     "shell.execute_reply": "2022-06-13T13:10:01.978254Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q torchsummary"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:10:01.985086Z",
     "iopub.execute_input": "2022-06-13T13:10:01.987426Z",
     "iopub.status.idle": "2022-06-13T13:10:13.182141Z",
     "shell.execute_reply.started": "2022-06-13T13:10:01.987386Z",
     "shell.execute_reply": "2022-06-13T13:10:13.181044Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import distutils.util\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def print_arguments(args):\n",
    "    print(\"-----------  Configuration Arguments -----------\")\n",
    "    for arg, value in sorted(vars(args).items()):\n",
    "        print(\"%s: %s\" % (arg, value))\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "\n",
    "def add_arguments(argname, type, default, help, argparser, **kwargs):\n",
    "    type = distutils.util.strtobool if type == bool else type\n",
    "    argparser.add_argument(\"--\" + argname,\n",
    "                           default=default,\n",
    "                           type=type,\n",
    "                           help=help + '',\n",
    "                           **kwargs)\n",
    "\n",
    "\n",
    "def cal_accuracy_threshold(y_score, y_true):\n",
    "    y_score = np.asarray(y_score)\n",
    "    y_true = np.asarray(y_true)\n",
    "    best_accuracy = 0\n",
    "    best_threshold = 0\n",
    "    for i in tqdm(range(0, 100)):\n",
    "        threshold = i * 0.01\n",
    "        y_test = (y_score >= threshold)\n",
    "        acc = np.mean((y_test == y_true).astype(int))\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_accuracy, best_threshold\n",
    "\n",
    "\n",
    "def cal_accuracy(y_score, y_true, threshold=0.5):\n",
    "    y_score = np.asarray(y_score)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_test = (y_score >= threshold)\n",
    "    accuracy = np.mean((y_test == y_true).astype(int))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def cosin_metric(x1, x2):\n",
    "    return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:10:13.183751Z",
     "iopub.execute_input": "2022-06-13T13:10:13.184506Z",
     "iopub.status.idle": "2022-06-13T13:10:13.198036Z",
     "shell.execute_reply.started": "2022-06-13T13:10:13.184458Z",
     "shell.execute_reply": "2022-06-13T13:10:13.197020Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q visualdl"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:10:13.200335Z",
     "iopub.execute_input": "2022-06-13T13:10:13.201251Z",
     "iopub.status.idle": "2022-06-13T13:10:26.945263Z",
     "shell.execute_reply.started": "2022-06-13T13:10:13.201211Z",
     "shell.execute_reply": "2022-06-13T13:10:26.944175Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmarkdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\ngym 0.23.1 requires importlib-metadata>=4.10.0; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchsummary import summary\n",
    "from visualdl import LogWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from modules.loss import AAMLoss\n",
    "# from modules.ecapa_tdnn import EcapaTdnn, SpeakerIdetification\n",
    "# from data_utils.reader import CustomDataset, collate_fn\n",
    "# from data_utils.noise_perturb import NoisePerturbAugmentor\n",
    "# from data_utils.speed_perturb import SpeedPerturbAugmentor\n",
    "# from data_utils.volume_perturb import VolumePerturbAugmentor\n",
    "# from data_utils.spec_augment import SpecAugmentor\n",
    "# from utils.utility import add_arguments, print_arguments\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=__doc__)\n",
    "add_arg = functools.partial(add_arguments, argparser=parser)\n",
    "add_arg('gpus',             str,    '0',                      )\n",
    "add_arg('use_model',        str,    'ecapa_tdnn',             )\n",
    "add_arg('batch_size',       int,    256,                      )\n",
    "add_arg('num_workers',      int,    4,                        )\n",
    "add_arg('num_epoch',        int,    100,                      )\n",
    "add_arg('num_speakers',     int,    2,                        )\n",
    "add_arg('learning_rate',    float,  1e-3,                     )\n",
    "add_arg('save_model_dir',   str,    'models/',                )\n",
    "add_arg('feature_method',   str,    'melspectrogram'         , choices=['melspectrogram', 'spectrogram'])\n",
    "add_arg('augment_conf_path',str,    'configs/augment.yml',    )\n",
    "add_arg('resume',           str,    None,                     )\n",
    "add_arg('pretrained_model', str,    None,                     )\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, eval_loader):\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    device = torch.device(\"cuda\")\n",
    "    lbs = []\n",
    "    pred = []\n",
    "    \n",
    "    for batch_id, (audio, label, _) in enumerate(eval_loader):\n",
    "        audio = audio.to(device)\n",
    "        output = model(audio)\n",
    "        # 计算准确率\n",
    "        lbs+=label.cpu().tolist()\n",
    "        output = output.data.cpu().numpy()\n",
    "        output = np.argmax(output, axis=1)\n",
    "        pred+=output.tolist()\n",
    "        \n",
    "        label = label.data.cpu().numpy()\n",
    "        acc = np.mean((output == label).astype(int))\n",
    "        accuracies.append(acc.item())\n",
    "    model.train()\n",
    "    return float(sum(accuracies) / len(accuracies)) , pred , lbs\n",
    "\n",
    "def save_model(save_path, model, optimizer, epoch):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, 'model.pth'))\n",
    "    torch.save({'last_epoch': torch.tensor(epoch)}, os.path.join(save_path, 'model.state'))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_path, 'optimizer.pth'))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:16:49.989076Z",
     "iopub.execute_input": "2022-06-13T13:16:49.989711Z",
     "iopub.status.idle": "2022-06-13T13:16:50.021409Z",
     "shell.execute_reply.started": "2022-06-13T13:16:49.989664Z",
     "shell.execute_reply": "2022-06-13T13:16:50.020313Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lbs,fns = get_fn_lbs()\n",
    "print(\"Done\")\n",
    "train_fns, val_fns, train_lbs, val_lbs = train_test_split(fns, lbs, test_size=0.1, random_state=42,shuffle = True)\n",
    "train_fns, test_fns, train_lbs, test_lbs = train_test_split(fns, lbs, test_size=0.1, random_state=42,shuffle = True)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:10:27.120387Z",
     "iopub.execute_input": "2022-06-13T13:10:27.120761Z",
     "iopub.status.idle": "2022-06-13T13:11:36.258025Z",
     "shell.execute_reply.started": "2022-06-13T13:10:27.120725Z",
     "shell.execute_reply": "2022-06-13T13:11:36.256989Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "Done\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "acc_save_model = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
    "print_arguments(args)\n",
    "\n",
    "device_ids = [int(i) for i in args.gpus.split(',')]\n",
    "\n",
    "writer = LogWriter(logdir='log')\n",
    "\n",
    "augmentors = None\n",
    "#     if args.augment_conf_path is not None:\n",
    "#         augmentors = {}\n",
    "#         with open(args.augment_conf_path, encoding=\"utf-8\") as fp:\n",
    "#             configs = yaml.load(fp, Loader=yaml.FullLoader)\n",
    "#         augmentors['noise'] = NoisePerturbAugmentor(**configs['noise'])\n",
    "#         augmentors['speed'] = SpeedPerturbAugmentor(**configs['speed'])\n",
    "#         augmentors['volume'] = VolumePerturbAugmentor(**configs['volume'])\n",
    "#         augmentors['specaug'] = SpecAugmentor(**configs['specaug'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_fns,train_lbs,\n",
    "                              feature_method=args.feature_method,\n",
    "                              mode='train',\n",
    "                              sr=8000,\n",
    "                              chunk_duration=2,\n",
    "                              augmentors=augmentors)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=args.batch_size * len(device_ids),\n",
    "                          collate_fn=collate_fn,\n",
    "                          shuffle=True,\n",
    "                          num_workers=args.num_workers)\n",
    "\n",
    "eval_dataset = CustomDataset(val_fns,val_lbs,\n",
    "                             feature_method=args.feature_method,\n",
    "                             mode='eval',\n",
    "                             sr=8000,\n",
    "                             chunk_duration=2)\n",
    "eval_loader = DataLoader(dataset=eval_dataset,\n",
    "                         batch_size=args.batch_size,\n",
    "                         collate_fn=collate_fn,\n",
    "                         num_workers=args.num_workers)\n",
    "\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "if args.use_model == 'ecapa_tdnn':\n",
    "    ecapa_tdnn = EcapaTdnn(input_size=train_dataset.input_size)\n",
    "    model = Classification(backbone=ecapa_tdnn, num_class=args.num_speakers)\n",
    "else:\n",
    "    raise Exception(f'{args.use_model} 模型不存在！')\n",
    "\n",
    "if len(args.gpus.split(',')) > 1:\n",
    "    model = DataParallel(model, device_ids=device_ids, output_device=device_ids[0])\n",
    "\n",
    "model.to(device)\n",
    "if len(args.gpus.split(',')) > 1:\n",
    "    summary(model.module, (train_dataset.input_size, 98), device='cuda')\n",
    "else:\n",
    "    summary(model, (train_dataset.input_size, 98), device='cuda')\n",
    "\n",
    "\n",
    "last_epoch = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=5e-4)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=args.num_epoch)\n",
    "\n",
    "\n",
    "if args.pretrained_model is not None:\n",
    "    model_dict = model.state_dict()\n",
    "    param_state_dict = torch.load(os.path.join(args.pretrained_model, 'model.pth'))\n",
    "    for name, weight in model_dict.items():\n",
    "        if name in param_state_dict.keys():\n",
    "            if list(weight.shape) != list(param_state_dict[name].shape):\n",
    "                print('{} not used, shape {} unmatched with {} in model.'.\n",
    "                      format(name, list(param_state_dict[name].shape), list(weight.shape)))\n",
    "                param_state_dict.pop(name, None)\n",
    "        else:\n",
    "            print('Lack weight: {}'.format(name))\n",
    "    model.load_state_dict(param_state_dict, strict=False)\n",
    "\n",
    "\n",
    "\n",
    "if args.resume is not None:\n",
    "    model.load_state_dict(torch.load(os.path.join(args.resume, 'model.pth')))\n",
    "    state = torch.load(os.path.join(args.resume, 'model.state'))\n",
    "    last_epoch = state['last_epoch']\n",
    "    optimizer_state = torch.load(os.path.join(args.resume, 'optimizer.pth'))\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_step = 0\n",
    "test_step = 0\n",
    "sum_batch = len(train_loader) * (args.num_epoch - last_epoch)\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:16:53.401437Z",
     "iopub.execute_input": "2022-06-13T13:16:53.401836Z",
     "iopub.status.idle": "2022-06-13T13:16:53.539302Z",
     "shell.execute_reply.started": "2022-06-13T13:16:53.401805Z",
     "shell.execute_reply": "2022-06-13T13:16:53.538382Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": "-----------  Configuration Arguments -----------\naugment_conf_path: configs/augment.yml\nbatch_size: 256\nfeature_method: melspectrogram\ngpus: 0\nlearning_rate: 0.001\nnum_epoch: 100\nnum_speakers: 2\nnum_workers: 4\npretrained_model: None\nresume: None\nsave_model_dir: models/\ntest_list_path: dataset/test_list.txt\ntrain_list_path: dataset/train_list.txt\nuse_model: ecapa_tdnn\n------------------------------------------------\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv1d-1              [-1, 512, 98]         204,800\n       BatchNorm1d-2              [-1, 512, 98]           1,024\n      Conv1dReluBn-3              [-1, 512, 98]               0\n            Conv1d-4              [-1, 512, 98]         262,144\n       BatchNorm1d-5              [-1, 512, 98]           1,024\n      Conv1dReluBn-6              [-1, 512, 98]               0\n            Conv1d-7               [-1, 64, 98]          12,288\n       BatchNorm1d-8               [-1, 64, 98]             128\n            Conv1d-9               [-1, 64, 98]          12,288\n      BatchNorm1d-10               [-1, 64, 98]             128\n           Conv1d-11               [-1, 64, 98]          12,288\n      BatchNorm1d-12               [-1, 64, 98]             128\n           Conv1d-13               [-1, 64, 98]          12,288\n      BatchNorm1d-14               [-1, 64, 98]             128\n           Conv1d-15               [-1, 64, 98]          12,288\n      BatchNorm1d-16               [-1, 64, 98]             128\n           Conv1d-17               [-1, 64, 98]          12,288\n      BatchNorm1d-18               [-1, 64, 98]             128\n           Conv1d-19               [-1, 64, 98]          12,288\n      BatchNorm1d-20               [-1, 64, 98]             128\n Res2Conv1dReluBn-21              [-1, 512, 98]               0\n           Conv1d-22              [-1, 512, 98]         262,144\n      BatchNorm1d-23              [-1, 512, 98]           1,024\n     Conv1dReluBn-24              [-1, 512, 98]               0\n           Linear-25                  [-1, 256]         131,328\n           Linear-26                  [-1, 512]         131,584\n       SE_Connect-27              [-1, 512, 98]               0\n           Conv1d-28              [-1, 512, 98]         262,144\n      BatchNorm1d-29              [-1, 512, 98]           1,024\n     Conv1dReluBn-30              [-1, 512, 98]               0\n           Conv1d-31               [-1, 64, 98]          12,288\n      BatchNorm1d-32               [-1, 64, 98]             128\n           Conv1d-33               [-1, 64, 98]          12,288\n      BatchNorm1d-34               [-1, 64, 98]             128\n           Conv1d-35               [-1, 64, 98]          12,288\n      BatchNorm1d-36               [-1, 64, 98]             128\n           Conv1d-37               [-1, 64, 98]          12,288\n      BatchNorm1d-38               [-1, 64, 98]             128\n           Conv1d-39               [-1, 64, 98]          12,288\n      BatchNorm1d-40               [-1, 64, 98]             128\n           Conv1d-41               [-1, 64, 98]          12,288\n      BatchNorm1d-42               [-1, 64, 98]             128\n           Conv1d-43               [-1, 64, 98]          12,288\n      BatchNorm1d-44               [-1, 64, 98]             128\n Res2Conv1dReluBn-45              [-1, 512, 98]               0\n           Conv1d-46              [-1, 512, 98]         262,144\n      BatchNorm1d-47              [-1, 512, 98]           1,024\n     Conv1dReluBn-48              [-1, 512, 98]               0\n           Linear-49                  [-1, 256]         131,328\n           Linear-50                  [-1, 512]         131,584\n       SE_Connect-51              [-1, 512, 98]               0\n           Conv1d-52              [-1, 512, 98]         262,144\n      BatchNorm1d-53              [-1, 512, 98]           1,024\n     Conv1dReluBn-54              [-1, 512, 98]               0\n           Conv1d-55               [-1, 64, 98]          12,288\n      BatchNorm1d-56               [-1, 64, 98]             128\n           Conv1d-57               [-1, 64, 98]          12,288\n      BatchNorm1d-58               [-1, 64, 98]             128\n           Conv1d-59               [-1, 64, 98]          12,288\n      BatchNorm1d-60               [-1, 64, 98]             128\n           Conv1d-61               [-1, 64, 98]          12,288\n      BatchNorm1d-62               [-1, 64, 98]             128\n           Conv1d-63               [-1, 64, 98]          12,288\n      BatchNorm1d-64               [-1, 64, 98]             128\n           Conv1d-65               [-1, 64, 98]          12,288\n      BatchNorm1d-66               [-1, 64, 98]             128\n           Conv1d-67               [-1, 64, 98]          12,288\n      BatchNorm1d-68               [-1, 64, 98]             128\n Res2Conv1dReluBn-69              [-1, 512, 98]               0\n           Conv1d-70              [-1, 512, 98]         262,144\n      BatchNorm1d-71              [-1, 512, 98]           1,024\n     Conv1dReluBn-72              [-1, 512, 98]               0\n           Linear-73                  [-1, 256]         131,328\n           Linear-74                  [-1, 512]         131,584\n       SE_Connect-75              [-1, 512, 98]               0\n           Conv1d-76             [-1, 1536, 98]       2,360,832\n           Conv1d-77              [-1, 128, 98]         196,736\n           Conv1d-78             [-1, 1536, 98]         198,144\nAttentiveStatsPool-79                 [-1, 3072]               0\n      BatchNorm1d-80                 [-1, 3072]           6,144\n           Linear-81                  [-1, 192]         590,016\n      BatchNorm1d-82                  [-1, 192]             384\n        EcapaTdnn-83                  [-1, 192]               0\n          Dropout-84                  [-1, 192]               0\n================================================================\nTotal params: 6,186,560\nTrainable params: 6,186,560\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.03\nForward/backward pass size (MB): 14.81\nParams size (MB): 23.60\nEstimated Total Size (MB): 38.44\n----------------------------------------------------------------\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "for epoch in range(last_epoch, args.num_epoch):\n",
    "    loss_sum = []\n",
    "    accuracies = []\n",
    "    start = time.time()\n",
    "    for batch_id, (audio, label, _) in enumerate(train_loader):          \n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device).long()\n",
    "        output = model(audio)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.data.cpu().numpy()\n",
    "        output = np.argmax(output, axis=1)\n",
    "        label = label.data.cpu().numpy()\n",
    "        acc = np.mean((output == label).astype(int))\n",
    "        accuracies.append(acc.item())\n",
    "        loss_sum.append(loss.item())\n",
    "\n",
    "        if batch_id % 30 == 0:\n",
    "            eta_sec = ((time.time() - start) * 1000) * (sum_batch - (epoch - last_epoch) * len(train_loader) - batch_id)\n",
    "            eta_str = str(timedelta(seconds=int(eta_sec / 1000)))\n",
    "            print(f'[{datetime.now()}] '\n",
    "                  f'Train epoch [{epoch}/{args.num_epoch}], '\n",
    "                  f'batch: [{batch_id}/{len(train_loader)}], '\n",
    "                  f'loss: {(sum(loss_sum) / len(loss_sum)):.5f}, '\n",
    "                  f'accuracy: {(sum(accuracies) / len(accuracies)):.5f}, '\n",
    "                  f'lr: {scheduler.get_lr()[0]:.8f}, '\n",
    "                  f'eta: {eta_str}')\n",
    "            writer.add_scalar('Train/Loss', loss.item(), train_step)\n",
    "            writer.add_scalar('Train/Accuracy', (sum(accuracies) / len(accuracies)), train_step)\n",
    "            train_step += 1\n",
    "        start = time.time()\n",
    "\n",
    "    s = time.time()\n",
    "    acc, _ ,_ = evaluate(model, eval_loader)\n",
    "    eta_str = str(timedelta(seconds=int(time.time() - s)))\n",
    "    print('='*70)\n",
    "    print(f'[{datetime.now()}] Test {epoch}, accuracy: {acc:.5f} time: {eta_str}')\n",
    "    print('='*70)\n",
    "    writer.add_scalar('Test/Accuracy', acc, test_step)\n",
    "\n",
    "    writer.add_scalar('Train/Learning rate', scheduler.get_lr()[0], epoch)\n",
    "    test_step += 1\n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    save_path = os.path.join(args.save_model_dir, args.use_model)\n",
    "    \n",
    "    \n",
    "    if acc > acc_save_model:\n",
    "        save_path_1 = 'model.pt'\n",
    "        print(\"Save model acc>acc_save\" + str(acc))\n",
    "        acc_save_model = acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "        save_model(save_path_1, best_model, optimizer, epoch)\n",
    "        \n",
    "    if len(device_ids) > 1:\n",
    "        print(\"Save model de>1\")\n",
    "        save_model(save_path, model.module, optimizer, epoch)\n",
    "    else:\n",
    "        print(\"Save model de<1\")\n",
    "        save_model(save_path, model, optimizer, epoch)\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T13:16:54.650168Z",
     "iopub.execute_input": "2022-06-13T13:16:54.650768Z",
     "iopub.status.idle": "2022-06-13T16:02:00.156447Z",
     "shell.execute_reply.started": "2022-06-13T13:16:54.650730Z",
     "shell.execute_reply": "2022-06-13T16:02:00.155263Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "[2022-06-13 13:17:02.708153] Train epoch [0/100], batch: [0/52], loss: 0.69333, accuracy: 0.52734, lr: 0.00100000, eta: 11:37:00\n[2022-06-13 13:18:04.227575] Train epoch [0/100], batch: [30/52], loss: 0.34344, accuracy: 0.93284, lr: 0.00100000, eta: 0:47:28\n======================================================================\n[2022-06-13 13:18:57.201923] Test 0, accuracy: 0.94745 time: 0:00:15\n======================================================================\nSave model acc>acc_save0.9474500240847784\nSave model de<1\n[2022-06-13 13:19:06.159999] Train epoch [1/100], batch: [0/52], loss: 0.22104, accuracy: 0.98047, lr: 0.00099951, eta: 12:16:11\n[2022-06-13 13:19:58.579912] Train epoch [1/100], batch: [30/52], loss: 0.24485, accuracy: 0.95728, lr: 0.00099951, eta: 1:25:04\n======================================================================\n[2022-06-13 13:20:39.723473] Test 1, accuracy: 0.95331 time: 0:00:10\n======================================================================\nSave model acc>acc_save0.9533093990847784\nSave model de<1\n[2022-06-13 13:20:48.947786] Train epoch [2/100], batch: [0/52], loss: 0.22561, accuracy: 0.95703, lr: 0.00099827, eta: 12:26:41\n[2022-06-13 13:21:42.042546] Train epoch [2/100], batch: [30/52], loss: 0.21064, accuracy: 0.96510, lr: 0.00099827, eta: 5:41:39\n======================================================================\n[2022-06-13 13:22:22.148839] Test 2, accuracy: 0.94422 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:22:29.257072] Train epoch [3/100], batch: [0/52], loss: 0.21018, accuracy: 0.96875, lr: 0.00099655, eta: 9:27:34\n[2022-06-13 13:23:26.257278] Train epoch [3/100], batch: [30/52], loss: 0.20794, accuracy: 0.96421, lr: 0.00099655, eta: 0:34:58\n======================================================================\n[2022-06-13 13:24:07.560528] Test 3, accuracy: 0.95266 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 13:24:16.265059] Train epoch [4/100], batch: [0/52], loss: 0.20658, accuracy: 0.96484, lr: 0.00099434, eta: 11:46:13\n[2022-06-13 13:25:05.866674] Train epoch [4/100], batch: [30/52], loss: 0.19917, accuracy: 0.96913, lr: 0.00099434, eta: 0:22:44\n======================================================================\n[2022-06-13 13:25:48.481853] Test 4, accuracy: 0.93836 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 13:25:56.931945] Train epoch [5/100], batch: [0/52], loss: 0.20215, accuracy: 0.96875, lr: 0.00099164, eta: 11:17:49\n[2022-06-13 13:26:46.672622] Train epoch [5/100], batch: [30/52], loss: 0.19639, accuracy: 0.96913, lr: 0.00099164, eta: 4:39:19\n======================================================================\n[2022-06-13 13:27:29.944059] Test 5, accuracy: 0.91141 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 13:27:37.524289] Train epoch [6/100], batch: [0/52], loss: 0.18319, accuracy: 0.97266, lr: 0.00098845, eta: 10:00:23\n[2022-06-13 13:28:30.245457] Train epoch [6/100], batch: [30/52], loss: 0.19876, accuracy: 0.96774, lr: 0.00098845, eta: 0:33:28\n======================================================================\n[2022-06-13 13:29:14.481015] Test 6, accuracy: 0.94675 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:29:19.764275] Train epoch [7/100], batch: [0/52], loss: 0.20511, accuracy: 0.96484, lr: 0.00098478, eta: 6:47:52\n[2022-06-13 13:30:12.459186] Train epoch [7/100], batch: [30/52], loss: 0.20582, accuracy: 0.96371, lr: 0.00098478, eta: 1:01:12\n======================================================================\n[2022-06-13 13:30:54.784457] Test 7, accuracy: 0.94386 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:31:01.868160] Train epoch [8/100], batch: [0/52], loss: 0.20828, accuracy: 0.96484, lr: 0.00098064, eta: 9:07:40\n[2022-06-13 13:31:55.803993] Train epoch [8/100], batch: [30/52], loss: 0.20469, accuracy: 0.96585, lr: 0.00098064, eta: 5:06:12\n======================================================================\n[2022-06-13 13:32:36.937026] Test 8, accuracy: 0.96107 time: 0:00:10\n======================================================================\nSave model acc>acc_save0.9610692136319846\nSave model de<1\n[2022-06-13 13:32:44.536861] Train epoch [9/100], batch: [0/52], loss: 0.18940, accuracy: 0.97656, lr: 0.00097602, eta: 9:22:38\n[2022-06-13 13:33:45.583992] Train epoch [9/100], batch: [30/52], loss: 0.20080, accuracy: 0.96585, lr: 0.00097602, eta: 0:29:26\n======================================================================\n[2022-06-13 13:34:31.226402] Test 9, accuracy: 0.95651 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 13:34:40.623510] Train epoch [10/100], batch: [0/52], loss: 0.20131, accuracy: 0.96094, lr: 0.00097093, eta: 11:51:10\n[2022-06-13 13:35:33.460632] Train epoch [10/100], batch: [30/52], loss: 0.18985, accuracy: 0.97203, lr: 0.00097093, eta: 0:32:17\n======================================================================\n[2022-06-13 13:36:17.911223] Test 10, accuracy: 0.95685 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:36:27.532933] Train epoch [11/100], batch: [0/52], loss: 0.19977, accuracy: 0.96875, lr: 0.00096538, eta: 11:55:48\n[2022-06-13 13:37:17.243144] Train epoch [11/100], batch: [30/52], loss: 0.19257, accuracy: 0.97089, lr: 0.00096538, eta: 0:20:32\n======================================================================\n[2022-06-13 13:38:00.930476] Test 11, accuracy: 0.94935 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:38:07.687222] Train epoch [12/100], batch: [0/52], loss: 0.21533, accuracy: 0.95312, lr: 0.00095937, eta: 8:18:10\n[2022-06-13 13:39:04.200609] Train epoch [12/100], batch: [30/52], loss: 0.20077, accuracy: 0.96573, lr: 0.00095937, eta: 0:25:57\n======================================================================\n[2022-06-13 13:39:42.062076] Test 12, accuracy: 0.95000 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:39:49.940191] Train epoch [13/100], batch: [0/52], loss: 0.21068, accuracy: 0.96094, lr: 0.00095290, eta: 9:37:34\n[2022-06-13 13:40:42.601831] Train epoch [13/100], batch: [30/52], loss: 0.19085, accuracy: 0.97152, lr: 0.00095290, eta: 6:08:20\n======================================================================\n[2022-06-13 13:41:22.103497] Test 13, accuracy: 0.90555 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:41:32.034257] Train epoch [14/100], batch: [0/52], loss: 0.23600, accuracy: 0.94141, lr: 0.00094599, eta: 12:01:13\n[2022-06-13 13:42:29.380631] Train epoch [14/100], batch: [30/52], loss: 0.19600, accuracy: 0.96875, lr: 0.00094599, eta: 0:42:24\n======================================================================\n[2022-06-13 13:43:11.748487] Test 14, accuracy: 0.94193 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 13:43:18.374273] Train epoch [15/100], batch: [0/52], loss: 0.24210, accuracy: 0.94141, lr: 0.00093864, eta: 7:51:45\n[2022-06-13 13:44:15.768939] Train epoch [15/100], batch: [30/52], loss: 0.19081, accuracy: 0.97165, lr: 0.00093864, eta: 0:57:07\n======================================================================\n[2022-06-13 13:45:00.517981] Test 15, accuracy: 0.94516 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:45:09.378098] Train epoch [16/100], batch: [0/52], loss: 0.18246, accuracy: 0.97656, lr: 0.00093086, eta: 10:27:36\n[2022-06-13 13:46:05.483072] Train epoch [16/100], batch: [30/52], loss: 0.19317, accuracy: 0.97089, lr: 0.00093086, eta: 0:32:24\n======================================================================\n[2022-06-13 13:46:49.823410] Test 16, accuracy: 0.95563 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:46:58.764122] Train epoch [17/100], batch: [0/52], loss: 0.17938, accuracy: 0.98047, lr: 0.00092265, eta: 10:26:02\n[2022-06-13 13:47:54.125494] Train epoch [17/100], batch: [30/52], loss: 0.18881, accuracy: 0.97291, lr: 0.00092265, eta: 0:28:07\n======================================================================\n[2022-06-13 13:48:37.698517] Test 17, accuracy: 0.94906 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:48:47.238328] Train epoch [18/100], batch: [0/52], loss: 0.18637, accuracy: 0.97656, lr: 0.00091403, eta: 10:54:27\n[2022-06-13 13:49:40.020982] Train epoch [18/100], batch: [30/52], loss: 0.18930, accuracy: 0.97278, lr: 0.00091403, eta: 0:30:02\n======================================================================\n[2022-06-13 13:50:21.009581] Test 18, accuracy: 0.95388 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:50:29.082559] Train epoch [19/100], batch: [0/52], loss: 0.19099, accuracy: 0.96875, lr: 0.00090500, eta: 9:11:15\n[2022-06-13 13:51:20.499446] Train epoch [19/100], batch: [30/52], loss: 0.19077, accuracy: 0.97215, lr: 0.00090500, eta: 0:27:43\n======================================================================\n[2022-06-13 13:52:03.155506] Test 19, accuracy: 0.95229 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 13:52:12.158778] Train epoch [20/100], batch: [0/52], loss: 0.19271, accuracy: 0.96875, lr: 0.00089557, eta: 10:08:26\n[2022-06-13 13:53:03.836522] Train epoch [20/100], batch: [30/52], loss: 0.18985, accuracy: 0.97228, lr: 0.00089557, eta: 0:21:13\n======================================================================\n[2022-06-13 13:53:49.120065] Test 20, accuracy: 0.95266 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 13:53:57.901646] Train epoch [21/100], batch: [0/52], loss: 0.20826, accuracy: 0.96094, lr: 0.00088574, eta: 9:46:06\n[2022-06-13 13:54:50.119124] Train epoch [21/100], batch: [30/52], loss: 0.19527, accuracy: 0.96888, lr: 0.00088574, eta: 0:29:41\n======================================================================\n[2022-06-13 13:55:33.438422] Test 21, accuracy: 0.95073 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:55:41.945399] Train epoch [22/100], batch: [0/52], loss: 0.24388, accuracy: 0.94141, lr: 0.00087554, eta: 9:19:04\n[2022-06-13 13:56:36.877547] Train epoch [22/100], batch: [30/52], loss: 0.19829, accuracy: 0.96862, lr: 0.00087554, eta: 0:23:26\n======================================================================\n[2022-06-13 13:57:20.189093] Test 22, accuracy: 0.95682 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 13:57:29.827063] Train epoch [23/100], batch: [0/52], loss: 0.16598, accuracy: 0.98438, lr: 0.00086497, eta: 10:27:48\n[2022-06-13 13:58:25.111073] Train epoch [23/100], batch: [30/52], loss: 0.18637, accuracy: 0.97379, lr: 0.00086497, eta: 0:27:02\n======================================================================\n[2022-06-13 13:59:07.658479] Test 23, accuracy: 0.95912 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 13:59:17.120597] Train epoch [24/100], batch: [0/52], loss: 0.19884, accuracy: 0.96484, lr: 0.00085404, eta: 10:08:00\n[2022-06-13 14:00:12.525903] Train epoch [24/100], batch: [30/52], loss: 0.18680, accuracy: 0.97366, lr: 0.00085404, eta: 0:20:27\n======================================================================\n[2022-06-13 14:00:58.709202] Test 24, accuracy: 0.91532 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:01:07.191526] Train epoch [25/100], batch: [0/52], loss: 0.18449, accuracy: 0.97266, lr: 0.00084276, eta: 8:52:30\n[2022-06-13 14:02:00.666654] Train epoch [25/100], batch: [30/52], loss: 0.18622, accuracy: 0.97429, lr: 0.00084276, eta: 0:23:55\n======================================================================\n[2022-06-13 14:02:45.622439] Test 25, accuracy: 0.95654 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:02:52.501046] Train epoch [26/100], batch: [0/52], loss: 0.19252, accuracy: 0.97266, lr: 0.00083114, eta: 7:06:26\n[2022-06-13 14:03:48.347199] Train epoch [26/100], batch: [30/52], loss: 0.18756, accuracy: 0.97329, lr: 0.00083114, eta: 0:21:07\n======================================================================\n[2022-06-13 14:04:32.907726] Test 26, accuracy: 0.95524 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:04:40.487753] Train epoch [27/100], batch: [0/52], loss: 0.15921, accuracy: 0.99219, lr: 0.00081920, eta: 7:45:17\n[2022-06-13 14:05:37.078427] Train epoch [27/100], batch: [30/52], loss: 0.18486, accuracy: 0.97505, lr: 0.00081920, eta: 5:07:19\n======================================================================\n[2022-06-13 14:06:22.532750] Test 27, accuracy: 0.94742 time: 0:00:12\n======================================================================\nSave model de<1\n[2022-06-13 14:06:33.841539] Train epoch [28/100], batch: [0/52], loss: 0.19255, accuracy: 0.96875, lr: 0.00080694, eta: 11:30:18\n[2022-06-13 14:07:29.265975] Train epoch [28/100], batch: [30/52], loss: 0.21048, accuracy: 0.96006, lr: 0.00080694, eta: 3:01:43\n======================================================================\n[2022-06-13 14:08:23.350926] Test 28, accuracy: 0.95979 time: 0:00:13\n======================================================================\nSave model de<1\n[2022-06-13 14:08:31.759971] Train epoch [29/100], batch: [0/52], loss: 0.19158, accuracy: 0.96875, lr: 0.00079438, eta: 8:22:46\n[2022-06-13 14:09:32.902631] Train epoch [29/100], batch: [30/52], loss: 0.19024, accuracy: 0.97190, lr: 0.00079438, eta: 0:29:30\n======================================================================\n[2022-06-13 14:10:20.130063] Test 29, accuracy: 0.95815 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:10:27.975809] Train epoch [30/100], batch: [0/52], loss: 0.19238, accuracy: 0.97266, lr: 0.00078153, eta: 7:33:47\n[2022-06-13 14:11:39.570417] Train epoch [30/100], batch: [30/52], loss: 0.19481, accuracy: 0.96925, lr: 0.00078153, eta: 7:20:08\n======================================================================\n[2022-06-13 14:12:20.139891] Test 30, accuracy: 0.95781 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:12:28.165261] Train epoch [31/100], batch: [0/52], loss: 0.17298, accuracy: 0.98438, lr: 0.00076840, eta: 7:46:09\n[2022-06-13 14:13:26.396946] Train epoch [31/100], batch: [30/52], loss: 0.18468, accuracy: 0.97518, lr: 0.00076840, eta: 0:25:01\n======================================================================\n[2022-06-13 14:14:16.754264] Test 31, accuracy: 0.95781 time: 0:00:12\n======================================================================\nSave model de<1\n[2022-06-13 14:14:24.966195] Train epoch [32/100], batch: [0/52], loss: 0.16866, accuracy: 0.98438, lr: 0.00075501, eta: 7:48:43\n[2022-06-13 14:17:20.211934] Train epoch [33/100], batch: [30/52], loss: 0.18009, accuracy: 0.97770, lr: 0.00074136, eta: 0:15:53\n======================================================================\n[2022-06-13 14:18:07.559142] Test 33, accuracy: 0.95685 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:18:17.170502] Train epoch [34/100], batch: [0/52], loss: 0.18409, accuracy: 0.97656, lr: 0.00072748, eta: 8:56:03\n[2022-06-13 14:19:14.740148] Train epoch [34/100], batch: [30/52], loss: 0.18532, accuracy: 0.97455, lr: 0.00072748, eta: 0:32:27\n======================================================================\n[2022-06-13 14:20:01.757088] Test 34, accuracy: 0.95651 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:20:09.085899] Train epoch [35/100], batch: [0/52], loss: 0.19103, accuracy: 0.97266, lr: 0.00071337, eta: 6:39:11\n[2022-06-13 14:21:13.608857] Train epoch [35/100], batch: [30/52], loss: 0.20245, accuracy: 0.96598, lr: 0.00071337, eta: 0:25:10\n======================================================================\n[2022-06-13 14:21:53.233023] Test 35, accuracy: 0.96107 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:22:01.720891] Train epoch [36/100], batch: [0/52], loss: 0.21819, accuracy: 0.95312, lr: 0.00069906, eta: 7:38:53\n[2022-06-13 14:22:53.856813] Train epoch [36/100], batch: [30/52], loss: 0.18966, accuracy: 0.97303, lr: 0.00069906, eta: 0:22:04\n======================================================================\n[2022-06-13 14:23:35.403598] Test 36, accuracy: 0.95977 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:23:41.921272] Train epoch [37/100], batch: [0/52], loss: 0.18243, accuracy: 0.97656, lr: 0.00068455, eta: 5:43:54\n[2022-06-13 14:24:32.249839] Train epoch [37/100], batch: [30/52], loss: 0.17849, accuracy: 0.97908, lr: 0.00068455, eta: 0:21:08\n======================================================================\n[2022-06-13 14:25:14.026860] Test 37, accuracy: 0.94612 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:25:22.630819] Train epoch [38/100], batch: [0/52], loss: 0.17035, accuracy: 0.98047, lr: 0.00066985, eta: 7:30:52\n[2022-06-13 14:26:12.538756] Train epoch [38/100], batch: [30/52], loss: 0.18232, accuracy: 0.97669, lr: 0.00066985, eta: 0:19:49\n======================================================================\n[2022-06-13 14:26:52.706505] Test 38, accuracy: 0.95102 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:27:00.413596] Train epoch [39/100], batch: [0/52], loss: 0.16183, accuracy: 0.98828, lr: 0.00065499, eta: 6:34:35\n[2022-06-13 14:27:52.900410] Train epoch [39/100], batch: [30/52], loss: 0.18362, accuracy: 0.97543, lr: 0.00065499, eta: 0:27:28\n======================================================================\n[2022-06-13 14:28:32.509155] Test 39, accuracy: 0.95427 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:28:41.674486] Train epoch [40/100], batch: [0/52], loss: 0.18359, accuracy: 0.97656, lr: 0.00063998, eta: 7:45:11\n[2022-06-13 14:29:34.140389] Train epoch [40/100], batch: [30/52], loss: 0.17874, accuracy: 0.97858, lr: 0.00063998, eta: 0:17:28\n======================================================================\n[2022-06-13 14:30:10.703060] Test 40, accuracy: 0.96138 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:30:20.021727] Train epoch [41/100], batch: [0/52], loss: 0.16613, accuracy: 0.98828, lr: 0.00062483, eta: 7:45:07\n[2022-06-13 14:31:12.267680] Train epoch [41/100], batch: [30/52], loss: 0.18063, accuracy: 0.97858, lr: 0.00062483, eta: 0:26:24\n======================================================================\n[2022-06-13 14:31:50.579170] Test 41, accuracy: 0.95849 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:31:58.237261] Train epoch [42/100], batch: [0/52], loss: 0.20686, accuracy: 0.96094, lr: 0.00060955, eta: 6:13:42\n[2022-06-13 14:32:50.069088] Train epoch [42/100], batch: [30/52], loss: 0.17689, accuracy: 0.98047, lr: 0.00060955, eta: 0:27:49\n======================================================================\n[2022-06-13 14:33:30.023583] Test 42, accuracy: 0.96011 time: 0:00:11\n======================================================================\nSave model de<1\n[2022-06-13 14:33:36.564090] Train epoch [43/100], batch: [0/52], loss: 0.16437, accuracy: 0.98828, lr: 0.00059417, eta: 5:12:25\n[2022-06-13 14:34:25.138208] Train epoch [43/100], batch: [30/52], loss: 0.17891, accuracy: 0.97883, lr: 0.00059417, eta: 0:22:07\n======================================================================\n[2022-06-13 14:35:07.725175] Test 43, accuracy: 0.95847 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:35:13.618193] Train epoch [44/100], batch: [0/52], loss: 0.16500, accuracy: 0.98828, lr: 0.00057870, eta: 4:35:26\n[2022-06-13 14:36:05.774116] Train epoch [44/100], batch: [30/52], loss: 0.17563, accuracy: 0.97996, lr: 0.00057870, eta: 0:18:09\n======================================================================\n[2022-06-13 14:36:45.663716] Test 44, accuracy: 0.96042 time: 0:00:12\n======================================================================\nSave model de<1\n[2022-06-13 14:36:51.984101] Train epoch [45/100], batch: [0/52], loss: 0.15888, accuracy: 0.98828, lr: 0.00056315, eta: 4:50:57\n[2022-06-13 14:37:40.241735] Train epoch [45/100], batch: [30/52], loss: 0.17685, accuracy: 0.97971, lr: 0.00056315, eta: 1:00:43\n======================================================================\n[2022-06-13 14:38:20.997741] Test 45, accuracy: 0.96078 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:38:27.970698] Train epoch [46/100], batch: [0/52], loss: 0.17808, accuracy: 0.98047, lr: 0.00054753, eta: 5:16:01\n[2022-06-13 14:39:16.051692] Train epoch [46/100], batch: [30/52], loss: 0.17611, accuracy: 0.98022, lr: 0.00054753, eta: 0:13:46\n======================================================================\n[2022-06-13 14:39:58.213054] Test 46, accuracy: 0.96107 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:40:06.125647] Train epoch [47/100], batch: [0/52], loss: 0.19289, accuracy: 0.97266, lr: 0.00053187, eta: 5:42:41\n[2022-06-13 14:40:56.402701] Train epoch [47/100], batch: [30/52], loss: 0.17362, accuracy: 0.98173, lr: 0.00053187, eta: 0:15:02\n======================================================================\n[2022-06-13 14:41:32.298581] Test 47, accuracy: 0.96102 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 14:41:40.657379] Train epoch [48/100], batch: [0/52], loss: 0.17019, accuracy: 0.98438, lr: 0.00051618, eta: 6:06:56\n[2022-06-13 14:42:29.096009] Train epoch [48/100], batch: [30/52], loss: 0.17468, accuracy: 0.98085, lr: 0.00051618, eta: 0:17:45\n======================================================================\n[2022-06-13 14:43:10.309945] Test 48, accuracy: 0.96073 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:43:17.638638] Train epoch [49/100], batch: [0/52], loss: 0.20131, accuracy: 0.96484, lr: 0.00050048, eta: 5:14:04\n[2022-06-13 14:44:06.444021] Train epoch [49/100], batch: [30/52], loss: 0.17072, accuracy: 0.98324, lr: 0.00050048, eta: 0:14:53\n======================================================================\n[2022-06-13 14:44:44.949131] Test 49, accuracy: 0.95912 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 14:44:51.900713] Train epoch [50/100], batch: [0/52], loss: 0.16831, accuracy: 0.98438, lr: 0.00048477, eta: 4:51:44\n[2022-06-13 14:45:40.355209] Train epoch [50/100], batch: [30/52], loss: 0.18455, accuracy: 0.97581, lr: 0.00048477, eta: 1:52:59\n======================================================================\n[2022-06-13 14:46:18.389884] Test 50, accuracy: 0.96396 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:46:25.434211] Train epoch [51/100], batch: [0/52], loss: 0.16854, accuracy: 0.98438, lr: 0.00046908, eta: 4:49:35\n[2022-06-13 14:47:15.801890] Train epoch [51/100], batch: [30/52], loss: 0.17373, accuracy: 0.98122, lr: 0.00046908, eta: 0:15:46\n======================================================================\n[2022-06-13 14:47:54.100009] Test 51, accuracy: 0.96399 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:48:02.689829] Train epoch [52/100], batch: [0/52], loss: 0.16864, accuracy: 0.98438, lr: 0.00045342, eta: 5:48:16\n[2022-06-13 14:48:48.167169] Train epoch [52/100], batch: [30/52], loss: 0.17027, accuracy: 0.98374, lr: 0.00045342, eta: 0:37:06\n======================================================================\n[2022-06-13 14:49:27.160943] Test 52, accuracy: 0.96333 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:49:34.697029] Train epoch [53/100], batch: [0/52], loss: 0.19158, accuracy: 0.97266, lr: 0.00043781, eta: 4:58:04\n[2022-06-13 14:50:21.119469] Train epoch [53/100], batch: [30/52], loss: 0.17200, accuracy: 0.98236, lr: 0.00043781, eta: 0:14:26\n======================================================================\n[2022-06-13 14:51:01.156695] Test 53, accuracy: 0.96169 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:51:10.008922] Train epoch [54/100], batch: [0/52], loss: 0.17748, accuracy: 0.98047, lr: 0.00042226, eta: 5:44:16\n[2022-06-13 14:51:57.805790] Train epoch [54/100], batch: [30/52], loss: 0.17122, accuracy: 0.98311, lr: 0.00042226, eta: 0:15:22\n======================================================================\n[2022-06-13 14:52:37.691253] Test 54, accuracy: 0.96271 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:52:44.904401] Train epoch [55/100], batch: [0/52], loss: 0.17580, accuracy: 0.98047, lr: 0.00040679, eta: 4:32:36\n[2022-06-13 14:53:32.088008] Train epoch [55/100], batch: [30/52], loss: 0.17037, accuracy: 0.98337, lr: 0.00040679, eta: 0:13:53\n======================================================================\n[2022-06-13 14:54:10.518425] Test 55, accuracy: 0.96169 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 14:54:19.275031] Train epoch [56/100], batch: [0/52], loss: 0.15724, accuracy: 0.99219, lr: 0.00039140, eta: 5:25:35\n[2022-06-13 14:55:06.080912] Train epoch [56/100], batch: [30/52], loss: 0.16381, accuracy: 0.98715, lr: 0.00039140, eta: 0:14:36\n======================================================================\n[2022-06-13 14:55:46.063980] Test 56, accuracy: 0.95943 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 14:55:53.227891] Train epoch [57/100], batch: [0/52], loss: 0.19176, accuracy: 0.97266, lr: 0.00037613, eta: 4:18:33\n[2022-06-13 14:56:42.507630] Train epoch [57/100], batch: [30/52], loss: 0.17218, accuracy: 0.98211, lr: 0.00037613, eta: 0:13:33\n======================================================================\n[2022-06-13 14:57:20.938566] Test 57, accuracy: 0.96591 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 14:57:30.687123] Train epoch [58/100], batch: [0/52], loss: 0.14860, accuracy: 0.99609, lr: 0.00036098, eta: 5:46:42\n[2022-06-13 14:58:16.925147] Train epoch [58/100], batch: [30/52], loss: 0.16454, accuracy: 0.98690, lr: 0.00036098, eta: 0:14:33\n======================================================================\n[2022-06-13 14:58:56.704516] Test 58, accuracy: 0.95943 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 14:59:03.923492] Train epoch [59/100], batch: [0/52], loss: 0.15533, accuracy: 0.99219, lr: 0.00034596, eta: 4:08:28\n[2022-06-13 14:59:51.879079] Train epoch [59/100], batch: [30/52], loss: 0.17017, accuracy: 0.98438, lr: 0.00034596, eta: 0:13:09\n======================================================================\n[2022-06-13 15:00:31.794449] Test 59, accuracy: 0.95620 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:00:36.913983] Train epoch [60/100], batch: [0/52], loss: 0.16346, accuracy: 0.98828, lr: 0.00033110, eta: 2:49:47\n[2022-06-13 15:01:30.578193] Train epoch [60/100], batch: [30/52], loss: 0.16574, accuracy: 0.98639, lr: 0.00033110, eta: 2:05:47\n======================================================================\n[2022-06-13 15:02:07.537849] Test 60, accuracy: 0.95847 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:02:14.701245] Train epoch [61/100], batch: [0/52], loss: 0.16305, accuracy: 0.98828, lr: 0.00031641, eta: 3:54:35\n[2022-06-13 15:03:01.612575] Train epoch [61/100], batch: [30/52], loss: 0.16745, accuracy: 0.98538, lr: 0.00031641, eta: 0:12:43\n======================================================================\n[2022-06-13 15:03:39.207901] Test 61, accuracy: 0.96234 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:03:46.491523] Train epoch [62/100], batch: [0/52], loss: 0.20630, accuracy: 0.95703, lr: 0.00030190, eta: 3:52:36\n[2022-06-13 15:04:36.982335] Train epoch [62/100], batch: [30/52], loss: 0.17105, accuracy: 0.98299, lr: 0.00030190, eta: 0:13:29\n======================================================================\n[2022-06-13 15:05:18.845110] Test 62, accuracy: 0.96297 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:05:26.811755] Train epoch [63/100], batch: [0/52], loss: 0.16612, accuracy: 0.98438, lr: 0.00028758, eta: 4:08:18\n[2022-06-13 15:06:15.771961] Train epoch [63/100], batch: [30/52], loss: 0.17158, accuracy: 0.98274, lr: 0.00028758, eta: 0:40:50\n======================================================================\n[2022-06-13 15:06:54.388501] Test 63, accuracy: 0.96495 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:07:02.604780] Train epoch [64/100], batch: [0/52], loss: 0.16994, accuracy: 0.98438, lr: 0.00027347, eta: 4:05:23\n[2022-06-13 15:07:49.148417] Train epoch [64/100], batch: [30/52], loss: 0.16499, accuracy: 0.98652, lr: 0.00027347, eta: 0:22:51\n======================================================================\n[2022-06-13 15:08:31.208019] Test 64, accuracy: 0.96591 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:08:38.455676] Train epoch [65/100], batch: [0/52], loss: 0.18162, accuracy: 0.97656, lr: 0.00025959, eta: 3:33:17\n[2022-06-13 15:09:25.916847] Train epoch [65/100], batch: [30/52], loss: 0.16362, accuracy: 0.98715, lr: 0.00025959, eta: 0:11:23\n======================================================================\n[2022-06-13 15:10:02.878939] Test 65, accuracy: 0.96656 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:10:10.168233] Train epoch [66/100], batch: [0/52], loss: 0.15988, accuracy: 0.98828, lr: 0.00024595, eta: 3:28:29\n[2022-06-13 15:10:54.537374] Train epoch [66/100], batch: [30/52], loss: 0.16317, accuracy: 0.98753, lr: 0.00024595, eta: 0:11:42\n======================================================================\n[2022-06-13 15:11:34.756746] Test 66, accuracy: 0.96399 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:11:40.591992] Train epoch [67/100], batch: [0/52], loss: 0.17802, accuracy: 0.98047, lr: 0.00023255, eta: 2:40:27\n[2022-06-13 15:12:29.724218] Train epoch [67/100], batch: [30/52], loss: 0.16551, accuracy: 0.98589, lr: 0.00023255, eta: 0:10:01\n======================================================================\n[2022-06-13 15:13:06.600700] Test 67, accuracy: 0.96654 time: 0:00:08\n======================================================================\nSave model de<1\n[2022-06-13 15:13:13.557211] Train epoch [68/100], batch: [0/52], loss: 0.16723, accuracy: 0.98438, lr: 0.00021942, eta: 3:06:55\n[2022-06-13 15:13:57.584996] Train epoch [68/100], batch: [30/52], loss: 0.16364, accuracy: 0.98740, lr: 0.00021942, eta: 0:08:11\n======================================================================\n[2022-06-13 15:14:34.718557] Test 68, accuracy: 0.96753 time: 0:00:09\n======================================================================\nSave model acc>acc_save0.9675269448458574\nSave model de<1\n[2022-06-13 15:14:41.399990] Train epoch [69/100], batch: [0/52], loss: 0.16389, accuracy: 0.98828, lr: 0.00020657, eta: 2:48:09\n[2022-06-13 15:15:28.836753] Train epoch [69/100], batch: [30/52], loss: 0.16750, accuracy: 0.98513, lr: 0.00020657, eta: 0:19:57\n======================================================================\n[2022-06-13 15:16:09.391215] Test 69, accuracy: 0.96268 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:16:17.183020] Train epoch [70/100], batch: [0/52], loss: 0.16510, accuracy: 0.98828, lr: 0.00019401, eta: 3:16:40\n[2022-06-13 15:17:09.541719] Train epoch [70/100], batch: [30/52], loss: 0.16546, accuracy: 0.98652, lr: 0.00019401, eta: 2:28:42\n======================================================================\n[2022-06-13 15:17:45.234884] Test 70, accuracy: 0.96625 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:17:53.120923] Train epoch [71/100], batch: [0/52], loss: 0.16648, accuracy: 0.98438, lr: 0.00018175, eta: 3:12:40\n[2022-06-13 15:18:38.911541] Train epoch [71/100], batch: [30/52], loss: 0.16386, accuracy: 0.98740, lr: 0.00018175, eta: 0:09:00\n======================================================================\n[2022-06-13 15:19:16.980652] Test 71, accuracy: 0.96333 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:19:25.195455] Train epoch [72/100], batch: [0/52], loss: 0.17458, accuracy: 0.98047, lr: 0.00016981, eta: 3:14:03\n[2022-06-13 15:20:10.003689] Train epoch [72/100], batch: [30/52], loss: 0.16448, accuracy: 0.98702, lr: 0.00016981, eta: 0:09:55\n======================================================================\n[2022-06-13 15:20:49.006026] Test 72, accuracy: 0.96656 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:20:55.919883] Train epoch [73/100], batch: [0/52], loss: 0.14818, accuracy: 0.99609, lr: 0.00015819, eta: 2:36:44\n[2022-06-13 15:21:43.926801] Train epoch [73/100], batch: [30/52], loss: 0.16148, accuracy: 0.98879, lr: 0.00015819, eta: 0:06:11\n======================================================================\n[2022-06-13 15:22:22.605853] Test 73, accuracy: 0.96982 time: 0:00:11\n======================================================================\nSave model acc>acc_save0.9698187620423893\nSave model de<1\n[2022-06-13 15:22:31.342817] Train epoch [74/100], batch: [0/52], loss: 0.17327, accuracy: 0.98047, lr: 0.00014691, eta: 3:07:09\n[2022-06-13 15:23:19.204742] Train epoch [74/100], batch: [30/52], loss: 0.16435, accuracy: 0.98690, lr: 0.00014691, eta: 0:08:30\n======================================================================\n[2022-06-13 15:23:55.229163] Test 74, accuracy: 0.95617 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:24:02.879274] Train epoch [75/100], batch: [0/52], loss: 0.18912, accuracy: 0.97266, lr: 0.00013597, eta: 2:41:11\n[2022-06-13 15:24:48.988971] Train epoch [75/100], batch: [30/52], loss: 0.16255, accuracy: 0.98790, lr: 0.00013597, eta: 0:07:23\n======================================================================\n[2022-06-13 15:25:27.687248] Test 75, accuracy: 0.96333 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:25:35.361949] Train epoch [76/100], batch: [0/52], loss: 0.17017, accuracy: 0.98438, lr: 0.00012540, eta: 2:34:57\n[2022-06-13 15:26:21.708820] Train epoch [76/100], batch: [30/52], loss: 0.16290, accuracy: 0.98765, lr: 0.00012540, eta: 0:07:20\n======================================================================\n[2022-06-13 15:26:58.019796] Test 76, accuracy: 0.96396 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:27:07.980072] Train epoch [77/100], batch: [0/52], loss: 0.16249, accuracy: 0.98828, lr: 0.00011520, eta: 3:14:00\n[2022-06-13 15:27:56.059209] Train epoch [77/100], batch: [30/52], loss: 0.16465, accuracy: 0.98677, lr: 0.00011520, eta: 0:07:15\n======================================================================\n[2022-06-13 15:28:34.549366] Test 77, accuracy: 0.96656 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:28:41.219103] Train epoch [78/100], batch: [0/52], loss: 0.17746, accuracy: 0.98047, lr: 0.00010538, eta: 2:02:46\n[2022-06-13 15:29:30.816289] Train epoch [78/100], batch: [30/52], loss: 0.16370, accuracy: 0.98727, lr: 0.00010538, eta: 1:57:46\n======================================================================\n[2022-06-13 15:30:05.285500] Test 78, accuracy: 0.96495 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:30:13.723357] Train epoch [79/100], batch: [0/52], loss: 0.15457, accuracy: 0.99219, lr: 0.00009594, eta: 2:29:44\n[2022-06-13 15:30:58.326807] Train epoch [79/100], batch: [30/52], loss: 0.16485, accuracy: 0.98664, lr: 0.00009594, eta: 0:07:16\n======================================================================\n[2022-06-13 15:31:37.619961] Test 79, accuracy: 0.96396 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:31:46.154234] Train epoch [80/100], batch: [0/52], loss: 0.16187, accuracy: 0.98828, lr: 0.00008691, eta: 2:24:10\n[2022-06-13 15:32:31.117529] Train epoch [80/100], batch: [30/52], loss: 0.16419, accuracy: 0.98727, lr: 0.00008691, eta: 0:05:44\n======================================================================\n[2022-06-13 15:33:06.890695] Test 80, accuracy: 0.96560 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:33:12.530193] Train epoch [81/100], batch: [0/52], loss: 0.16945, accuracy: 0.98438, lr: 0.00007828, eta: 1:29:07\n[2022-06-13 15:33:58.870266] Train epoch [81/100], batch: [30/52], loss: 0.16394, accuracy: 0.98715, lr: 0.00007828, eta: 0:22:01\n======================================================================\n[2022-06-13 15:34:36.121725] Test 81, accuracy: 0.96625 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:34:42.378403] Train epoch [82/100], batch: [0/52], loss: 0.15843, accuracy: 0.98828, lr: 0.00007007, eta: 1:33:52\n[2022-06-13 15:35:25.187828] Train epoch [82/100], batch: [30/52], loss: 0.16145, accuracy: 0.98853, lr: 0.00007007, eta: 0:54:18\n======================================================================\n[2022-06-13 15:36:01.433706] Test 82, accuracy: 0.96755 time: 0:00:08\n======================================================================\nSave model de<1\n[2022-06-13 15:36:10.563816] Train epoch [83/100], batch: [0/52], loss: 0.16174, accuracy: 0.98828, lr: 0.00006229, eta: 2:11:17\n[2022-06-13 15:36:53.438621] Train epoch [83/100], batch: [30/52], loss: 0.16147, accuracy: 0.98866, lr: 0.00006229, eta: 0:04:42\n======================================================================\n[2022-06-13 15:37:31.008991] Test 83, accuracy: 0.96399 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:37:38.903194] Train epoch [84/100], batch: [0/52], loss: 0.17001, accuracy: 0.98438, lr: 0.00005493, eta: 1:46:05\n[2022-06-13 15:38:21.992779] Train epoch [84/100], batch: [30/52], loss: 0.16452, accuracy: 0.98702, lr: 0.00005493, eta: 0:04:55\n======================================================================\n[2022-06-13 15:39:00.929364] Test 84, accuracy: 0.96594 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:39:08.288593] Train epoch [85/100], batch: [0/52], loss: 0.20098, accuracy: 0.96875, lr: 0.00004802, eta: 1:32:37\n[2022-06-13 15:39:53.756815] Train epoch [85/100], batch: [30/52], loss: 0.16436, accuracy: 0.98715, lr: 0.00004802, eta: 0:23:10\n======================================================================\n[2022-06-13 15:40:29.090111] Test 85, accuracy: 0.96591 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:40:36.137171] Train epoch [86/100], batch: [0/52], loss: 0.15468, accuracy: 0.99219, lr: 0.00004155, eta: 1:22:49\n[2022-06-13 15:41:23.132134] Train epoch [86/100], batch: [30/52], loss: 0.16092, accuracy: 0.98891, lr: 0.00004155, eta: 0:22:35\n======================================================================\n[2022-06-13 15:42:01.102330] Test 86, accuracy: 0.96529 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:42:06.474458] Train epoch [87/100], batch: [0/52], loss: 0.16074, accuracy: 0.98828, lr: 0.00003554, eta: 0:58:03\n[2022-06-13 15:42:57.415217] Train epoch [87/100], batch: [30/52], loss: 0.16023, accuracy: 0.98929, lr: 0.00003554, eta: 1:01:17\n======================================================================\n[2022-06-13 15:43:31.042345] Test 87, accuracy: 0.96755 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:43:38.337709] Train epoch [88/100], batch: [0/52], loss: 0.14039, accuracy: 1.00000, lr: 0.00002998, eta: 1:13:35\n[2022-06-13 15:44:26.498784] Train epoch [88/100], batch: [30/52], loss: 0.16105, accuracy: 0.98879, lr: 0.00002998, eta: 0:03:26\n======================================================================\n[2022-06-13 15:45:04.500497] Test 88, accuracy: 0.96464 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:45:11.892222] Train epoch [89/100], batch: [0/52], loss: 0.14103, accuracy: 1.00000, lr: 0.00002489, eta: 1:08:22\n[2022-06-13 15:45:58.962534] Train epoch [89/100], batch: [30/52], loss: 0.16266, accuracy: 0.98816, lr: 0.00002489, eta: 0:04:15\n======================================================================\n[2022-06-13 15:46:36.001302] Test 89, accuracy: 0.96399 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:46:44.247211] Train epoch [90/100], batch: [0/52], loss: 0.17803, accuracy: 0.98047, lr: 0.00002026, eta: 1:09:33\n[2022-06-13 15:47:31.809643] Train epoch [90/100], batch: [30/52], loss: 0.16044, accuracy: 0.98916, lr: 0.00002026, eta: 0:23:29\n======================================================================\n[2022-06-13 15:48:10.685734] Test 90, accuracy: 0.96365 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:48:19.102657] Train epoch [91/100], batch: [0/52], loss: 0.21410, accuracy: 0.96094, lr: 0.00001611, eta: 1:03:53\n[2022-06-13 15:49:04.635746] Train epoch [91/100], batch: [30/52], loss: 0.16108, accuracy: 0.98891, lr: 0.00001611, eta: 0:02:58\n======================================================================\n[2022-06-13 15:49:46.044433] Test 91, accuracy: 0.96625 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:49:53.235584] Train epoch [92/100], batch: [0/52], loss: 0.17616, accuracy: 0.98047, lr: 0.00001243, eta: 0:48:20\n[2022-06-13 15:50:40.667194] Train epoch [92/100], batch: [30/52], loss: 0.16086, accuracy: 0.98904, lr: 0.00001243, eta: 0:04:51\n======================================================================\n[2022-06-13 15:51:20.012863] Test 92, accuracy: 0.96690 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:51:26.421962] Train epoch [93/100], batch: [0/52], loss: 0.14794, accuracy: 0.99609, lr: 0.00000923, eta: 0:37:32\n[2022-06-13 15:52:15.366059] Train epoch [93/100], batch: [30/52], loss: 0.16236, accuracy: 0.98803, lr: 0.00000923, eta: 0:01:39\n======================================================================\n[2022-06-13 15:52:53.885624] Test 93, accuracy: 0.96787 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:52:59.286199] Train epoch [94/100], batch: [0/52], loss: 0.16299, accuracy: 0.98828, lr: 0.00000651, eta: 0:26:55\n[2022-06-13 15:53:50.357166] Train epoch [94/100], batch: [30/52], loss: 0.16182, accuracy: 0.98853, lr: 0.00000651, eta: 0:05:58\n======================================================================\n[2022-06-13 15:54:27.069994] Test 94, accuracy: 0.96721 time: 0:00:10\n======================================================================\nSave model de<1\n[2022-06-13 15:56:52.249996] Train epoch [96/100], batch: [30/52], loss: 0.15847, accuracy: 0.99005, lr: 0.00000253, eta: 0:14:58\n======================================================================\n[2022-06-13 15:57:28.025053] Test 96, accuracy: 0.96399 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:57:35.521686] Train epoch [97/100], batch: [0/52], loss: 0.16080, accuracy: 0.98828, lr: 0.00000125, eta: 0:18:55\n[2022-06-13 15:58:19.741433] Train epoch [97/100], batch: [30/52], loss: 0.16145, accuracy: 0.98879, lr: 0.00000125, eta: 0:05:50\n======================================================================\n[2022-06-13 15:58:57.345062] Test 97, accuracy: 0.96591 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 15:59:05.135654] Train epoch [98/100], batch: [0/52], loss: 0.15520, accuracy: 0.99219, lr: 0.00000044, eta: 0:13:07\n[2022-06-13 15:59:50.485890] Train epoch [98/100], batch: [30/52], loss: 0.15997, accuracy: 0.98954, lr: 0.00000044, eta: 0:00:30\n======================================================================\n[2022-06-13 16:00:28.138497] Test 98, accuracy: 0.96625 time: 0:00:09\n======================================================================\nSave model de<1\n[2022-06-13 16:00:35.799193] Train epoch [99/100], batch: [0/52], loss: 0.14001, accuracy: 1.00000, lr: 0.00000006, eta: 0:06:26\n[2022-06-13 16:01:20.227701] Train epoch [99/100], batch: [30/52], loss: 0.16029, accuracy: 0.98942, lr: 0.00000006, eta: 0:00:20\n======================================================================\n[2022-06-13 16:01:59.935946] Test 99, accuracy: 0.96560 time: 0:00:09\n======================================================================\nSave model de<1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "test_dataset = CustomDataset(test_fns,test_lbs,\n",
    "                             feature_method=args.feature_method,\n",
    "                             mode='eval',\n",
    "                             sr=8000,\n",
    "                             chunk_duration=2)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=args.batch_size,\n",
    "                         collate_fn=collate_fn,\n",
    "                         num_workers=args.num_workers)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T16:05:14.674601Z",
     "iopub.execute_input": "2022-06-13T16:05:14.675214Z",
     "iopub.status.idle": "2022-06-13T16:05:14.680294Z",
     "shell.execute_reply.started": "2022-06-13T16:05:14.675178Z",
     "shell.execute_reply": "2022-06-13T16:05:14.679367Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submit_lbs,submit_fns = get_fn_submit()\n",
    "submit_dataset = CustomDataset(submit_fns,submit_lbs,\n",
    "                             feature_method=args.feature_method,\n",
    "                             mode='eval',\n",
    "                             sr=8000,\n",
    "                             chunk_duration=2)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=args.batch_size,\n",
    "                         collate_fn=collate_fn,shuffle=False,\n",
    "                         num_workers=args.num_workers)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T16:05:25.192167Z",
     "iopub.execute_input": "2022-06-13T16:05:25.192993Z",
     "iopub.status.idle": "2022-06-13T16:05:25.221272Z",
     "shell.execute_reply.started": "2022-06-13T16:05:25.192957Z",
     "shell.execute_reply": "2022-06-13T16:05:25.220177Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_33/4254959474.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m submit_dataset = CustomDataset(submit_fns,submit_lbs,\n\u001B[0m\u001B[1;32m      2\u001B[0m                              \u001B[0mfeature_method\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature_method\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                              \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'eval'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                              \u001B[0msr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8000\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                              chunk_duration=2)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'submit_fns' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'submit_fns' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "acc, pred, lbs = evaluate(model, test_loader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T16:05:34.038747Z",
     "iopub.execute_input": "2022-06-13T16:05:34.039511Z",
     "iopub.status.idle": "2022-06-13T16:05:44.323657Z",
     "shell.execute_reply.started": "2022-06-13T16:05:34.039478Z",
     "shell.execute_reply": "2022-06-13T16:05:44.322590Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(lbs,pred,digits=4))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T16:05:56.050816Z",
     "iopub.execute_input": "2022-06-13T16:05:56.051199Z",
     "iopub.status.idle": "2022-06-13T16:05:56.067320Z",
     "shell.execute_reply.started": "2022-06-13T16:05:56.051170Z",
     "shell.execute_reply": "2022-06-13T16:05:56.066159Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": "              precision    recall  f1-score   support\n\n           0     0.9662    0.9620    0.9641       684\n           1     0.9663    0.9701    0.9682       769\n\n    accuracy                         0.9663      1453\n   macro avg     0.9663    0.9660    0.9662      1453\nweighted avg     0.9663    0.9663    0.9663      1453\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(acc)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T16:06:04.374178Z",
     "iopub.execute_input": "2022-06-13T16:06:04.374832Z",
     "iopub.status.idle": "2022-06-13T16:06:04.379667Z",
     "shell.execute_reply.started": "2022-06-13T16:06:04.374795Z",
     "shell.execute_reply": "2022-06-13T16:06:04.378868Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": "0.9656001625722542\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "acc, pred, lbs = evaluate(best_model, test_loader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T16:06:32.357700Z",
     "iopub.execute_input": "2022-06-13T16:06:32.358085Z",
     "iopub.status.idle": "2022-06-13T16:06:41.673058Z",
     "shell.execute_reply.started": "2022-06-13T16:06:32.358054Z",
     "shell.execute_reply": "2022-06-13T16:06:41.671922Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(lbs,pred,digits=4))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-13T16:06:41.677959Z",
     "iopub.execute_input": "2022-06-13T16:06:41.678722Z",
     "iopub.status.idle": "2022-06-13T16:06:41.703299Z",
     "shell.execute_reply.started": "2022-06-13T16:06:41.678674Z",
     "shell.execute_reply": "2022-06-13T16:06:41.702250Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": "              precision    recall  f1-score   support\n\n           0     0.9652    0.9722    0.9687       684\n           1     0.9751    0.9688    0.9720       769\n\n    accuracy                         0.9704      1453\n   macro avg     0.9701    0.9705    0.9703      1453\nweighted avg     0.9704    0.9704    0.9704      1453\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# acc, pred, lbs = evaluate(best_model, test_loader)\n",
    "# print(acc)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-06-10T18:10:36.093176Z",
     "iopub.status.idle": "2022-06-10T18:10:36.101686Z",
     "shell.execute_reply.started": "2022-06-10T18:10:36.101424Z",
     "shell.execute_reply": "2022-06-10T18:10:36.101455Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}